{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimSiam_modell_u14.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJtPOBKFDeopX9LAilTGzt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IIF0403/Thesis/blob/main/SimSiam_modell_u14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd07YtQtkrm3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "import random"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGCgMeDHklIM"
      },
      "source": [
        "##### DATA #######\n",
        "\n",
        "### Function to load a UCR time_series dataset from my github ###\n",
        "\n",
        "def load_dataset(Dataset):\n",
        "  #Dataset: The name of the dataset to load. Example: \"ECG5000\"\n",
        "\n",
        "  #github URL\n",
        "  url_raw = 'https://raw.githubusercontent.com/IIF0403/Thesis/main/data/'\n",
        "  url_train = url_raw + Dataset+'/'+Dataset+'_TRAIN'\n",
        "  url_test = url_raw + Dataset+'/'+Dataset+'_TEST'\n",
        "\n",
        "  #Loading the data\n",
        "  data_train = pd.read_csv(url_train,header=None)\n",
        "  data_test = pd.read_csv(url_test, header=None)\n",
        "  data = pd.concat((data_train, data_test))\n",
        "\n",
        "  #Want all datasets to have classes as integers starting from 0 \n",
        "  Y = data.values[:,0]\n",
        "  classes = len(np.unique(Y))\n",
        "  Y_transformed = ( (Y-Y.min())/(Y.max()-Y.min()) )*(classes-1)\n",
        "  data[data.columns[0]] = Y_transformed\n",
        "\n",
        "  #Inserting the name of the dataset as a column (for later use, when several datasets will be combined)\n",
        "  data.insert(loc=0, column = \"Dataset\", value=Dataset) \n",
        "\n",
        "  #Inserting the length of the time series T as a column (for later use)\n",
        "  T = data.shape[1]-2 #The length of the time series\n",
        "  data.insert(loc=1, column = \"T\", value = T) \n",
        "\n",
        "  return data, classes\n",
        "\n",
        "\n",
        "### Function to make a big dataset out of several datasets loaded from github ###\n",
        "def make_big_dataset(Datasets):\n",
        "  #Datasets: A list of the name of the datasets to load. Example: [\"ECG5000\", \"FordA\", \"FordB\"]\n",
        "\n",
        "  #Loading first dataset\n",
        "  data, classes =  load_dataset(Datasets[0])\n",
        "  used_classes = classes #keeping track of the class-labels already used\n",
        "\n",
        "  #Loading each of the datasets in the list and combining them all togehter in a big dataset\n",
        "  for i in range(1, len(Datasets)):\n",
        "    #loading the i´th dataset\n",
        "    Dataset = Datasets[i]\n",
        "    dataset, classes= load_dataset(Dataset)\n",
        "\n",
        "    #Need to change the class-labels such that all the class-labels of the different datasets differ from eachother\n",
        "    labels = dataset.values[:,2]\n",
        "    transformed_labels = labels + used_classes \n",
        "    dataset[dataset.columns[2]] = transformed_labels\n",
        "    \n",
        "    used_classes += classes #keeping track of the class-labels already used\n",
        "\n",
        "    data = pd.concat((data, dataset)) #adding the new dataset to the big dataset\n",
        "  \n",
        "  return data, used_classes\n",
        "\n",
        "\n",
        "\n",
        "### Dataset class ###\n",
        "class Timeseries_Dataset(Dataset):\n",
        "  def __init__(self, Datasets, Drive=False, Save=None, transform=None):\n",
        "    #Datasets: a list with the name of the datasets, can be a list og several or one dataset. If it is several datasets, they will be made into one big dataset\n",
        "    #Drive: True means loading the already saved dataset from google drive, false means loading data from github\n",
        "    #Save: name of new dataset, if we want to save the new dataset to google drive\n",
        "    #transform: a given transformation of the data\n",
        "\n",
        "    ##Loading the data\n",
        "    #if the data is saved in Google Drive, load the data from Drive\n",
        "    if (Drive == True):\n",
        "      Dataset = Datasets[0]\n",
        "      print(\"Loading '\",Dataset,\"' from Google Drive\")\n",
        "      #drive.mount(\"/content/gdrive\")\n",
        "      #data = pd.read_csv('/content/gdrive/My Drive/Datasets/'+Dataset+'.csv')\n",
        "\n",
        "      drive.mount('/content/drive')\n",
        "      data = pd.read_csv('/content/drive/MyDrive/Datasets/'+Dataset+'.csv')\n",
        "\n",
        "      classes = len(np.unique(data.values[:,2]))\n",
        "    \n",
        "    #else load data from github\n",
        "    else:\n",
        "      #If \"Datasets\" contains several datasets then make a big dataset \n",
        "      if (len(Datasets)>1):\n",
        "        print(\"Loading and combining \", Datasets,\" from github\")\n",
        "        data, classes = make_big_dataset(Datasets)\n",
        "      \n",
        "      #If \"Datasets\" only contains one dataset, load the dataset from github\n",
        "      else:\n",
        "        Dataset = Datasets[0]\n",
        "        print(\"Loading '\",Dataset,\"' from github\")\n",
        "        data, classes = load_dataset(Dataset)\n",
        "    \n",
        "    #Save new dataset to Google drive as 'name' if Save = 'name' and not None\n",
        "    if (Save!=None): #Save the new dataset to google drive as Save\n",
        "      print(\"Saving new dataset to google drive\")\n",
        "      #drive.mount(\"/content/gdrive\")\n",
        "      #data.to_csv('/content/gdrive/My Drive/Datasets/'+Save+'.csv', index=False)\n",
        "\n",
        "      drive.mount('/content/drive')\n",
        "      data.to_csv('/content/drive/MyDrive/Datasets/'+Save+'.csv', index=False)\n",
        "    \n",
        "    self.dataframe = data\n",
        "    self.transform = transform\n",
        "    self.classes = classes\n",
        "    self.Datasets = Datasets\n",
        "\n",
        "    \n",
        "  #defining the len(Dataset) function\n",
        "  def __len__(self): \n",
        "    return len(self.dataframe)\n",
        "\n",
        "  #defining the _getitem_ function which creates samples, such that when Dataset[i] is called; the i´th sample is returned\n",
        "  def __getitem__(self, key): \n",
        "\n",
        "    if isinstance(key, slice): # if given a slicing\n",
        "      start, stop, step = key.indices(len(self))\n",
        "      sliced = deepcopy(self)\n",
        "      sliced_data = self.dataframe.iloc[start:stop:step]\n",
        "      sliced.dataframe = sliced_data\n",
        "      return  sliced\n",
        "\n",
        "    else: #If given a single index \n",
        "      if torch.is_tensor(key):\n",
        "        key = key.tolist()\n",
        "      \n",
        "      #For one sample Dataset[i]:\n",
        "      dataframe = self.dataframe\n",
        "\n",
        "      label = dataframe.iloc[key, 2] #retrieveing the label\n",
        "      dataset = dataframe.iloc[key,0] #retrieveing the dataset-name\n",
        "      T = dataframe.iloc[key,1] #retrieveing the timeseries-length\n",
        "      time_series_with_nan = dataframe.iloc[key,3:].to_numpy() #retrieveing the timeseries (containing nan-values at the end)\n",
        "      time_series = time_series_with_nan[:T] #Removing nan_values at the end\n",
        "\n",
        "      sample = {'time_series': time_series, 'label': label, 'dataset': dataset, \"T\": T} #a sample is one timeseries with it's corresponding label (and som xtra information)\n",
        "\n",
        "      if self.transform: #transform sample\n",
        "        sample = self.transform(sample)\n",
        "\n",
        "      return sample\n",
        "  \n",
        "  def info(self):#Function to print information about the dataset\n",
        "    print(\"Datasets included: \", self.Datasets)\n",
        "    print(\"Number of classes : \", self.classes)\n",
        "    print(\"Size of dataset: \", len(self))\n",
        "  \n",
        "  def shuffle(self):\n",
        "    self.dataframe = self.dataframe.sample(frac = 1)\n",
        "  \n",
        "\n",
        "### Transformation class; segment timeseries into two augmentations\n",
        "class TwoSegments(object):\n",
        "  def __init__(self, horizon=0.3, window_gap=1, random_startpos = False, random_horizon=False, random_window_gap=False):\n",
        "    #horizon: horizon*T = window_length; the length of the two augmentations\n",
        "    #window_gap: the gap bewteen the two augmentations\n",
        "    #random_startpos: True means that the first augmentation starts at a random position\n",
        "    #random_horizon: True means that a random horizon is chosen\n",
        "    #random_window_gap: True means that a random window_gap is chosen\n",
        "\n",
        "    self.horizon = horizon\n",
        "    self.window_gap = window_gap\n",
        "    self.random_startpos = random_startpos\n",
        "    self.random_horizon = random_horizon\n",
        "    self.random_window_gap = random_window_gap\n",
        "        \n",
        "  def __call__(self, sample):\n",
        "\n",
        "    dataset = sample['dataset']\n",
        "    time_series = sample['time_series']\n",
        "    T = sample['T']\n",
        "    label = sample['label']\n",
        "\n",
        "    #horizon\n",
        "    if (self.random_horizon==True):\n",
        "      possible_horizons = [0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "      horizon = random.choice(possible_horizons) #draw a random horizon\n",
        "    else:\n",
        "      horizon = self.horizon\n",
        "\n",
        "    #window gap\n",
        "    if (self.random_window_gap==True):\n",
        "      possible_window_gaps =[0,1,2,3,4,5,6,7,8,9,10]\n",
        "      window_gap = random.choice(possible_window_gaps) #draw random window_gap\n",
        "    else:\n",
        "      window_gap = self.window_gap\n",
        "\n",
        "    window_length = int(horizon*T) #length of each augmentation\n",
        "    #window_length = 20\n",
        "\n",
        "    #finding start-position of the first augmentation\n",
        "    if (self.random_startpos == True): #if random start_position\n",
        "      max_possible_startposition = T-(2*window_length+window_gap) #the maximal start-position of the first augmentation\n",
        "      possible_startpossisions = [i for i in range(max_possible_startposition_aug1)] #The possible start positions of the first augmentation\n",
        "      start_pos = random.choice(possible_startpossisions) #draw a random startposition\n",
        "    else:\n",
        "      start_pos = 0 \n",
        "\n",
        "    #make the two augmentations of the timeseries\n",
        "    augmentation_1 = time_series[start_pos : (start_pos+window_length)]\n",
        "    augmentation_2 = time_series[(start_pos+window_length+window_gap) : (start_pos+window_length+window_gap)+window_length]\n",
        "\n",
        "    #create a new sample with the two augmentations\n",
        "    #new_sample = {'time_series': time_series, 'aug1': augmentation_1, 'aug2': augmentation_2, 'label': label, 'dataset': dataset, \"T\": T}\n",
        "    new_sample = {'aug1': augmentation_1, 'aug2': augmentation_2, 'label': label, 'dataset': dataset, \"T\": T}\n",
        "\n",
        "    return new_sample\n",
        "\n",
        "### Transformation class; convert into Tensor-data for PyTorch-use\n",
        "class ToTensor(object):\n",
        "  def __call__(self, sample):\n",
        "    dataset = sample['dataset']\n",
        "    T = sample['T']\n",
        "    label = sample['label']\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #if (len(sample)== 6):\n",
        "    if (len(sample)== 5):\n",
        "      #time_series = sample['time_series'].astype(float)\n",
        "      #time_series = time_series[np.newaxis,:] #tensor\n",
        "\n",
        "      aug1 = sample['aug1'].astype(float)\n",
        "      aug1 = aug1[np.newaxis,:] #tensor\n",
        "\n",
        "      aug2 = sample['aug2'].astype(float)\n",
        "      aug2 = aug2[np.newaxis,:] # tensor\n",
        "      \n",
        "      label = label.astype(float)\n",
        "\n",
        "      #time_series_tensor = torch.tensor(time_series, dtype=torch.float32, device=device)\n",
        "      aug1_tensor = torch.tensor(aug1, dtype=torch.float32, device=device)\n",
        "      aug2_tensor = torch.tensor(aug2, dtype=torch.float32, device=device)\n",
        "      label_tensor = torch.tensor(label, dtype=torch.long, device=device)\n",
        "\n",
        "      #torch_sample = {'time_series': time_series_tensor, 'aug1': aug1_tensor, 'aug2': aug2_tensor, 'label': label_tensor, 'dataset': dataset, \"T\": T}\n",
        "      torch_sample = { 'aug1': aug1_tensor, 'aug2': aug2_tensor, 'label': label_tensor, 'dataset': dataset, \"T\": T}\n",
        "\n",
        "\n",
        "    else:\n",
        "      time_series = sample['time_series'].astype(float)\n",
        "      label = label.astype(float)\n",
        "\n",
        "      time_series_tensor = torch.tensor(time_series, dtype=torch.float32, device=device)\n",
        "      label_tensor = torch.tensor(label, dtype=torch.long, device=device)\n",
        "\n",
        "      torch_sample = {'time_series': time_series_tensor, 'label': label_tensor, 'dataset': dataset, \"T\": T}\n",
        "\n",
        "    return torch_sample\n",
        "\n",
        "#Function to split into train and test dataset given a Timeseries_Dataset object\n",
        "def data_split(dat, train_size=0.8):\n",
        "  N = len(dat)\n",
        "  n = int(train_size*N)\n",
        "\n",
        "  dat.shuffle()\n",
        "  train = dat[:n]\n",
        "  test = dat[n:]\n",
        "\n",
        "  return train, test\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqSEyvR3oEhj"
      },
      "source": [
        "##### MODELS #######\n",
        "\n",
        "#FCN modellen fra prosjektoppgave\n",
        "class FCN(nn.Module):\n",
        "  def __init__(self):#, classes):\n",
        "    super(FCN, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(1, 128, 9, padding=(9 // 2))\n",
        "    self.bnorm1 = nn.BatchNorm1d(128)        \n",
        "    self.conv2 = nn.Conv1d(128, 256, 5, padding=(5 // 2))\n",
        "    self.bnorm2 = nn.BatchNorm1d(256)\n",
        "    self.conv3 = nn.Conv1d(256, 128, 3, padding=(3 // 2))\n",
        "    self.bnorm3 = nn.BatchNorm1d(128)        \n",
        "    #self.classification_head = nn.Linear(128, classes)\n",
        "    self.output_dim = 128\n",
        "\n",
        "  def forward(self, x):\n",
        "    b1_class = F.relu(self.bnorm1(self.conv1(x)))\n",
        "    b2_class = F.relu(self.bnorm2(self.conv2(b1_class)))\n",
        "    b3_class = F.relu(self.bnorm3(self.conv3(b2_class)))\n",
        "\n",
        "    features_class = torch.mean(b3_class, 2) \n",
        "    #out_class = self.classification_head(features_class)\n",
        "\n",
        "    return features_class"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIEQRdOkR8od"
      },
      "source": [
        "### SIMPLE SIAMESE REPRESENTATION LEANRING MODELLEN ###\n",
        "\n",
        "## DISTANCE FUNCTION ##\n",
        "#The distance to minimize  (Negative Cosine similarity)\n",
        "def D(p, z):\n",
        "  z = z.detach() #stop gradient\n",
        "  p = p\n",
        "  return - F.cosine_similarity(p, z, dim=1).mean()\n",
        "  #return F.cosine_similarity(p, z, dim=1).mean()\n",
        "\n",
        "\n",
        "## PROJECTION MLP ## ( in f(x))\n",
        "#has 3 FC layers with BN, the output FC has no ReLU, hidden FC is 2048-d\n",
        "class projection_MLP(nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim=2048, out_dim=2048): \n",
        "    super().__init__()\n",
        "    #Layer 1\n",
        "    self.FC1 = nn.Sequential(\n",
        "      nn.Linear(in_dim, hidden_dim),\n",
        "      nn.BatchNorm1d(hidden_dim),\n",
        "      nn.ReLU(inplace=True)  )\n",
        "    #Layer 2\n",
        "    self.FC2 = nn.Sequential(\n",
        "      nn.Linear(hidden_dim, hidden_dim),\n",
        "      nn.BatchNorm1d(hidden_dim),\n",
        "      nn.ReLU(inplace=True)  )\n",
        "    #Layer 3 (output)\n",
        "    self.FC3 = nn.Sequential(\n",
        "      nn.Linear(hidden_dim, out_dim),\n",
        "      nn.BatchNorm1d(hidden_dim) )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x1 = self.FC1(x)\n",
        "    x2 = self.FC2(x1)\n",
        "    x_out = self.FC3(x2)\n",
        "    return x_out\n",
        "\n",
        "\n",
        "## PREDICTION MLP ## h(z)\n",
        "#has 2 FC layers, only BN and ReLU on hidden layer (explained why in SimSiam paper). \n",
        "# input: z = f(x) dim = 2048\n",
        "# output: p = h(z) dim = 2048\n",
        "class prediction_MLP(nn.Module):\n",
        "  def __init__(self, in_dim=2048, hidden_dim=512, out_dim=2048): # bottleneck structure\n",
        "    super().__init__()\n",
        "    #Layer 1\n",
        "    self.FC1 = nn.Sequential(\n",
        "      nn.Linear(in_dim, hidden_dim),\n",
        "      nn.BatchNorm1d(hidden_dim),\n",
        "      nn.ReLU(inplace=True) )\n",
        "    #Layer 2 (output)\n",
        "    self.FC2 = nn.Linear(hidden_dim, out_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x1 = self.FC1(x)\n",
        "    x_out = self.FC2(x1)\n",
        "    return x_out\n",
        "\n",
        "\n",
        "## SIMSIAM MODEL ##\n",
        "class SimSiam(nn.Module):\n",
        "  def __init__(self, backbone=FCN()): #take backbone as input\n",
        "    super().__init__()\n",
        "    self.backbone = backbone\n",
        "    self.projector = projection_MLP(backbone.output_dim) \n",
        "    \n",
        "    #Encoder; z = f(x)\n",
        "    self.encoder = nn.Sequential( \n",
        "      self.backbone,\n",
        "      self.projector )\n",
        "\n",
        "    #Predictor; p = h(z)\n",
        "    self.predictor = prediction_MLP()\n",
        "  \n",
        "  def forward(self, x1, x2):\n",
        "    #x1, x2: augmentations of x\n",
        "    f = self.encoder\n",
        "    h = self.predictor\n",
        "\n",
        "    z1, z2 = f(x1), f(x2)\n",
        "    p1, p2 = h(z1), h(z2)\n",
        "\n",
        "    #Symmetric loss\n",
        "    Loss = D(p1,z2)/2 + D(p2,z1)/2 \n",
        "    return Loss\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssP27LrqjHtJ",
        "outputId": "84532a92-fa3c-4168-e457-4e8bc6d6bb9c"
      },
      "source": [
        "\n",
        "#Importing Dataset class from other ipynb file\n",
        "#!pip install import_ipynb\n",
        "#drive.mount('/content/drive')\n",
        "#%cd '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "#!ls\n",
        "#import import_ipynb\n",
        "#from Dataset_class import *\n",
        "\n",
        "Dataset_small = Datasets = [\"ChlorineConcentration\"]\n",
        "Small_dataset = Timeseries_Dataset(Dataset_small, transform = transforms.Compose( [TwoSegments(), ToTensor()] ))\n",
        "train_small, test_small = data_split(Small_dataset)\n",
        "\n",
        "#Dataset_big = [\"Big_dataset\"]\n",
        "#Big_dataset = Timeseries_Dataset(Dataset_big, Drive=True, transform = transforms.Compose( [TwoSegments(), ToTensor()] ))\n",
        "#train_big, test_big = data_split(Big_dataset)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading ' ChlorineConcentration ' from github\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLD7R9lzvKDZ"
      },
      "source": [
        "\n",
        "\n",
        "epochs = 10\n",
        "train_bs = 512\n",
        "momentum = 0.9\n",
        "weight_decay =0.0001\n",
        "warmup_epochs = 0 #=10 for for batch sizes  > 1024\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "base_lr = 0.05\n",
        "lr = (base_lr*train_bs)/256\n",
        "#lr= 0.0001\n",
        "\n",
        "model = SimSiam().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "train_loader = DataLoader(train_small, batch_size = train_bs, shuffle=True)\n",
        "losses =[]\n",
        "\n",
        "for e in range(epochs):\n",
        "  model.train()\n",
        "  for batch in enumerate(train_loader):\n",
        "    x1_batch = batch[1]['aug1']\n",
        "    x2_batch = batch[1]['aug2']\n",
        "\n",
        "    model.zero_grad()\n",
        "    loss = model.forward(x1_batch, x2_batch).mean()\n",
        "    #loss = model.forward(x1_batch.to(device,non_blocking=True), x2_batch.to(device,non_blocking=True)).mean()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "  \n",
        "  avg_loss = np.mean(losses)\n",
        "  print(\"e: \",e,\" Avg Loss: \", avg_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}