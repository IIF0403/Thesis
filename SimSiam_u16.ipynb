{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimSiam_u16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYf8+KN2RU1kLUC11rLBmL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IIF0403/Thesis/blob/main/SimSiam_u16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Kbtvs3gINM"
      },
      "source": [
        "#Check GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd07YtQtkrm3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "from google.colab import drive\n",
        "from torch.nn.utils.rnn import pack_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "import random\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGCgMeDHklIM"
      },
      "source": [
        "##### DATA #######\n",
        "\n",
        "### Function to load a UCR time_series dataset from my github ###\n",
        "def load_dataset(Dataset, train):\n",
        "  #Dataset: The name of the dataset to load. Example: \"ECG5000\"\n",
        "\n",
        "  #github URL\n",
        "  url_raw = 'https://raw.githubusercontent.com/IIF0403/Thesis/main/data/'\n",
        "  url_train = url_raw + Dataset+'/'+Dataset+'_TRAIN'\n",
        "  url_test = url_raw + Dataset+'/'+Dataset+'_TEST'\n",
        "\n",
        "  #Loading the data\n",
        "  #data_train = pd.read_csv(url_train,header=None)\n",
        "  #data_test = pd.read_csv(url_test, header=None)\n",
        "  #data = pd.concat((data_train, data_test))\n",
        "\n",
        "  if (train ==True): #bruker test_settene til train fordi de er større\n",
        "    data = pd.read_csv(url_test, header=None)\n",
        "  else:\n",
        "    data = pd.read_csv(url_train, header=None)\n",
        "\n",
        "  #Want all datasets to have classes as integers starting from 0 \n",
        "  Y = data.values[:,0]\n",
        "  classes = len(np.unique(Y))\n",
        "  Y_transformed = ( (Y-Y.min())/(Y.max()-Y.min()) )*(classes-1)\n",
        "  data[data.columns[0]] = Y_transformed\n",
        "\n",
        "  #Inserting the name of the dataset as a column (for later use, when several datasets will be combined)\n",
        "  data.insert(loc=0, column = \"Dataset\", value=Dataset) \n",
        "\n",
        "  #Inserting the length of the time series T as a column (for later use)\n",
        "  T = data.shape[1]-2 #The length of the time series\n",
        "  data.insert(loc=1, column = \"T\", value = T) \n",
        "\n",
        "  return data, classes\n",
        "\n",
        "\n",
        "### Function to make a big dataset out of several datasets loaded from github ###\n",
        "def make_big_dataset(Datasets, train):\n",
        "  #Datasets: A list of the name of the datasets to load. Example: [\"ECG5000\", \"FordA\", \"FordB\"]\n",
        "\n",
        "  #Loading first dataset\n",
        "  data, classes =  load_dataset(Datasets[0], train)\n",
        "  used_classes = classes #keeping track of the class-labels already used\n",
        "\n",
        "  #Loading each of the datasets in the list and combining them all togehter in a big dataset\n",
        "  for i in range(1, len(Datasets)):\n",
        "    #loading the i´th dataset\n",
        "    Dataset = Datasets[i]\n",
        "    dataset, classes= load_dataset(Dataset, train)\n",
        "\n",
        "    #Need to change the class-labels such that all the class-labels of the different datasets differ from eachother\n",
        "    labels = dataset.values[:,2]\n",
        "    transformed_labels = labels + used_classes \n",
        "    dataset[dataset.columns[2]] = transformed_labels\n",
        "    \n",
        "    used_classes += classes #keeping track of the class-labels already used\n",
        "\n",
        "    data = pd.concat((data, dataset)) #adding the new dataset to the big dataset\n",
        "  \n",
        "  return data, used_classes\n",
        "\n",
        "\n",
        "\n",
        "### Dataset class ###\n",
        "class Timeseries_Dataset(Dataset):\n",
        "  def __init__(self, Datasets, train = True, Drive=False, Save=None, transform=None):\n",
        "    #Datasets: a list with the name of the datasets, can be a list og several or one dataset. If it is several datasets, they will be made into one big dataset\n",
        "    #Drive: True means loading the already saved dataset from google drive, false means loading data from github\n",
        "    #Save: name of new dataset, if we want to save the new dataset to google drive\n",
        "    #transform: a given transformation of the data\n",
        "\n",
        "    ##Loading the data\n",
        "    #if the data is saved in Google Drive, load the data from Drive\n",
        "    if (Drive == True):\n",
        "      Dataset = Datasets[0]\n",
        "      print(\"Loading '\",Dataset,\"' from Google Drive\")\n",
        "      #drive.mount(\"/content/gdrive\")\n",
        "      #data = pd.read_csv('/content/gdrive/My Drive/Datasets/'+Dataset+'.csv')\n",
        "\n",
        "      drive.mount('/content/drive')\n",
        "      data = pd.read_csv('/content/drive/MyDrive/Datasets/'+Dataset+'.csv')\n",
        "\n",
        "      classes = len(np.unique(data.values[:,2]))\n",
        "    \n",
        "    #else load data from github\n",
        "    else:\n",
        "      #If \"Datasets\" contains several datasets then make a big dataset \n",
        "      if (len(Datasets)>1):\n",
        "        print(\"Loading and combining \", Datasets,\" from github\")\n",
        "        data, classes = make_big_dataset(Datasets, train)\n",
        "      \n",
        "      #If \"Datasets\" only contains one dataset, load the dataset from github\n",
        "      else:\n",
        "        Dataset = Datasets[0]\n",
        "        print(\"Loading '\",Dataset,\"' from github\")\n",
        "        data, classes = load_dataset(Dataset, train)\n",
        "    \n",
        "    #Save new dataset to Google drive as 'name' if Save = 'name' and not None\n",
        "    if (Save!=None): #Save the new dataset to google drive as Save\n",
        "      print(\"Saving new dataset to google drive\")\n",
        "      #drive.mount(\"/content/gdrive\")\n",
        "      #data.to_csv('/content/gdrive/My Drive/Datasets/'+Save+'.csv', index=False)\n",
        "\n",
        "      drive.mount('/content/drive')\n",
        "      data.to_csv('/content/drive/MyDrive/Datasets/'+Save+'.csv', index=False)\n",
        "    \n",
        "    self.dataframe = data\n",
        "    self.transform = transform\n",
        "    self.classes = classes\n",
        "    self.Datasets = Datasets\n",
        "\n",
        "    \n",
        "  #defining the len(Dataset) function\n",
        "  def __len__(self): \n",
        "    return len(self.dataframe)\n",
        "\n",
        "  #defining the _getitem_ function which creates samples, such that when Dataset[i] is called; the i´th sample is returned\n",
        "  def __getitem__(self, key): \n",
        "\n",
        "    if isinstance(key, slice): # if given a slicing\n",
        "      start, stop, step = key.indices(len(self))\n",
        "      sliced = deepcopy(self)\n",
        "      sliced_data = self.dataframe.iloc[start:stop:step]\n",
        "      sliced.dataframe = sliced_data\n",
        "      return  sliced\n",
        "\n",
        "    else: #If given a single index \n",
        "      if torch.is_tensor(key):\n",
        "        key = key.tolist()\n",
        "      \n",
        "      #For one sample Dataset[i]:\n",
        "      dataframe = self.dataframe\n",
        "\n",
        "      label = dataframe.iloc[key, 2] #retrieveing the label\n",
        "      dataset = dataframe.iloc[key,0] #retrieveing the dataset-name\n",
        "      T = dataframe.iloc[key,1] #retrieveing the timeseries-length\n",
        "      time_series_with_nan = dataframe.iloc[key,3:].to_numpy() #retrieveing the timeseries (containing nan-values at the end)\n",
        "      time_series = time_series_with_nan[:T] #Removing nan_values at the end\n",
        "\n",
        "      sample = {'time_series': time_series, 'label': label, 'dataset': dataset, \"T\": T} #a sample is one timeseries with it's corresponding label (and som xtra information)\n",
        "\n",
        "      if self.transform: #transform sample\n",
        "        sample = self.transform(sample)\n",
        "\n",
        "      return sample\n",
        "  \n",
        "  def info(self):#Function to print information about the dataset\n",
        "    print(\"Datasets included: \", self.Datasets)\n",
        "    print(\"Number of classes : \", self.classes)\n",
        "    print(\"Size of dataset: \", len(self))\n",
        "  \n",
        "  def shuffle(self):\n",
        "    self.dataframe = self.dataframe.sample(frac = 1)\n",
        "  \n",
        "\n",
        "### Transformation class; segment timeseries into two augmentations\n",
        "class TwoSegments(object):\n",
        "  def __init__(self, horizon=0.3, window_gap=1, random_startpos = False, random_horizon=False, random_window_gap=False):\n",
        "    #horizon: horizon*T = window_length; the length of the two augmentations\n",
        "    #window_gap: the gap bewteen the two augmentations\n",
        "    #random_startpos: True means that the first augmentation starts at a random position\n",
        "    #random_horizon: True means that a random horizon is chosen\n",
        "    #random_window_gap: True means that a random window_gap is chosen\n",
        "\n",
        "    self.horizon = horizon\n",
        "    self.window_gap = window_gap\n",
        "    self.random_startpos = random_startpos\n",
        "    self.random_horizon = random_horizon\n",
        "    self.random_window_gap = random_window_gap\n",
        "        \n",
        "  def __call__(self, sample):\n",
        "\n",
        "    dataset = sample['dataset']\n",
        "    time_series = sample['time_series']\n",
        "    T = sample['T']\n",
        "    label = sample['label']\n",
        "\n",
        "    #horizon\n",
        "    if (self.random_horizon==True):\n",
        "      possible_horizons = [0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "      horizon = random.choice(possible_horizons) #draw a random horizon\n",
        "    else:\n",
        "      horizon = self.horizon\n",
        "\n",
        "    #window gap\n",
        "    if (self.random_window_gap==True):\n",
        "      possible_window_gaps =[0,1,2,3,4,5,6,7,8,9,10]\n",
        "      window_gap = random.choice(possible_window_gaps) #draw random window_gap\n",
        "    else:\n",
        "      window_gap = self.window_gap\n",
        "\n",
        "    #window_length = int(horizon*T) #length of each augmentation\n",
        "    window_length = 38\n",
        "\n",
        "    #finding start-position of the first augmentation\n",
        "    if (self.random_startpos == True): #if random start_position\n",
        "      max_possible_startposition = T-(2*window_length+window_gap) #the maximal start-position of the first augmentation\n",
        "      possible_startpossisions = [i for i in range(max_possible_startposition_aug1)] #The possible start positions of the first augmentation\n",
        "      start_pos = random.choice(possible_startpossisions) #draw a random startposition\n",
        "    else:\n",
        "      start_pos = 0 \n",
        "\n",
        "    #make the two augmentations of the timeseries\n",
        "    augmentation_1 = time_series[start_pos : (start_pos+window_length)]\n",
        "    augmentation_2 = time_series[(start_pos+window_length+window_gap) : (start_pos+window_length+window_gap)+window_length]\n",
        "\n",
        "    #create a new sample with the two augmentations\n",
        "    #new_sample = {'time_series': time_series, 'aug1': augmentation_1, 'aug2': augmentation_2, 'label': label, 'dataset': dataset, \"T\": T}\n",
        "    new_sample = {'aug1': augmentation_1, 'aug2': augmentation_2, 'label': label, 'dataset': dataset, \"T\": T}\n",
        "\n",
        "    return new_sample\n",
        "\n",
        "### Transformation class; convert into Tensor-data for PyTorch-use\n",
        "class ToTensor(object):\n",
        "  def __call__(self, sample):\n",
        "    dataset = sample['dataset']\n",
        "    T = sample['T']\n",
        "    label = sample['label']\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #if (len(sample)== 6):\n",
        "    if (len(sample)== 5):\n",
        "      #time_series = sample['time_series'].astype(float)\n",
        "      #time_series = time_series[np.newaxis,:] #tensor\n",
        "\n",
        "      aug1 = sample['aug1'].astype(float)\n",
        "      aug1 = aug1[np.newaxis,:] #tensor\n",
        "\n",
        "      aug2 = sample['aug2'].astype(float)\n",
        "      aug2 = aug2[np.newaxis,:] # tensor\n",
        "      \n",
        "      #label = label.astype(float)\n",
        "\n",
        "      #time_series_tensor = torch.tensor(time_series, dtype=torch.float32, device=device)\n",
        "      aug1_tensor = torch.tensor(aug1, dtype=torch.float32, device=device)\n",
        "      aug2_tensor = torch.tensor(aug2, dtype=torch.float32, device=device)\n",
        "      label_tensor = torch.tensor(label, dtype=torch.long, device=device)\n",
        "\n",
        "      #torch_sample = {'time_series': time_series_tensor, 'aug1': aug1_tensor, 'aug2': aug2_tensor, 'label': label_tensor, 'dataset': dataset, \"T\": T}\n",
        "      torch_sample = {'aug1': aug1_tensor, 'aug2': aug2_tensor, 'label': label_tensor, 'dataset': dataset, \"T\": T}\n",
        "\n",
        "\n",
        "    else:\n",
        "      time_series = sample['time_series'].astype(float)\n",
        "      time_series = time_series[np.newaxis,:] #tensor\n",
        "\n",
        "      label = label.astype(float)\n",
        "\n",
        "      time_series_tensor = torch.tensor(time_series, dtype=torch.float32, device=device)\n",
        "      label_tensor = torch.tensor(label, dtype=torch.long, device=device)\n",
        "\n",
        "      torch_sample = {'time_series': time_series_tensor, 'label': label_tensor, 'dataset': dataset, \"T\": T}\n",
        "\n",
        "    return torch_sample\n",
        "\n",
        "#Function to split into train and test dataset given a Timeseries_Dataset object\n",
        "def data_split(dat, train_size=0.7):\n",
        "  N = len(dat)\n",
        "  n = int(train_size*N)\n",
        "\n",
        "  dat.shuffle()\n",
        "  train = dat[:n]\n",
        "  test = dat[n:]\n",
        "\n",
        "  return train, test\n",
        "\n",
        "#function to save results in Excel\n",
        "def to_Excel(Results, columns, output_name):\n",
        "  dataframe = pd.DataFrame(Results, columns = columns)\n",
        "  # create excel writer object\n",
        "  writer = pd.ExcelWriter(output_name)\n",
        "  # write dataframe to excel\n",
        "  dataframe.to_excel(writer)\n",
        "  # save the excel\n",
        "  writer.save()\n",
        "  files.download(output_name)\n",
        "  print('DataFrame is written successfully to Excel File.') \n",
        "\n",
        "###################\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqSEyvR3oEhj"
      },
      "source": [
        "##### MODELS #######\n",
        "\n",
        "### FCN modellen fra prosjektoppgave\n",
        "class FCN(nn.Module):\n",
        "  def __init__(self, class_train=False, classes = 0):\n",
        "    super(FCN, self).__init__()\n",
        "    self.class_train= class_train\n",
        "    self.classes = classes\n",
        "\n",
        "    self.conv1 = nn.Conv1d(1, 128, 9, padding=(9 // 2))\n",
        "    #self.bnorm1 = nn.BatchNorm1d(128) \n",
        "    self.bnorm1 = nn.GroupNorm(128,128)\n",
        "\n",
        "    self.conv2 = nn.Conv1d(128, 256, 5, padding=(5 // 2))\n",
        "    #self.bnorm2 = nn.BatchNorm1d(256)\n",
        "    self.bnorm2 = nn.GroupNorm(256,256)\n",
        "\n",
        "    self.conv3 = nn.Conv1d(256, 128, 3, padding=(3 // 2))\n",
        "    #self.bnorm3 = nn.BatchNorm1d(128)\n",
        "    self.bnorm3 = nn.GroupNorm(128,128)\n",
        "\n",
        "    self.output_dim = 128\n",
        "\n",
        "    if (class_train==True):\n",
        "      self.classification_head = nn.Linear(128, classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    b1_class = F.relu(self.bnorm1(self.conv1(x)))\n",
        "    b2_class = F.relu(self.bnorm2(self.conv2(b1_class)))\n",
        "    b3_class = F.relu(self.bnorm3(self.conv3(b2_class)))\n",
        "\n",
        "    features_class = torch.mean(b3_class, 2) \n",
        "\n",
        "    if (self.class_train==True):\n",
        "      out_class = self.classification_head(features_class)\n",
        "      return out_class\n",
        "    else:\n",
        "      return features_class\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIEQRdOkR8od"
      },
      "source": [
        "### SIMPLE SIAMESE REPRESENTATION LEANRING MODELLEN ###\n",
        "\n",
        "## DISTANCE FUNCTION ##\n",
        "#The distance to minimize  (Negative Cosine similarity)\n",
        "def D(p, z):\n",
        "  z = z.detach() #stop gradient\n",
        "  p = p\n",
        "\n",
        "  neg_cosine_sim = - F.cosine_similarity(p, z, dim=1) #Negative cosine similarities (size: [bs])\n",
        "\n",
        "  return neg_cosine_sim.mean()  #return the mean of the negative cosine similarities\n",
        "\n",
        "\n",
        "## PROJECTION MLP ## ( in f(x))\n",
        "#has 3 FC layers with BN, the output FC has no ReLU, hidden FC is 2048-d\n",
        "class projection_MLP(nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim=2048, out_dim=2048): \n",
        "    super().__init__()\n",
        "    #Layer 1\n",
        "    self.FC1 = nn.Sequential(\n",
        "      nn.Linear(in_dim, hidden_dim),\n",
        "      nn.BatchNorm1d(hidden_dim),\n",
        "      nn.ReLU(inplace=True)  )\n",
        "    #Layer 2\n",
        "    self.FC2 = nn.Sequential(\n",
        "      nn.Linear(hidden_dim, hidden_dim),\n",
        "      nn.BatchNorm1d(hidden_dim),\n",
        "      nn.ReLU(inplace=True)  )\n",
        "    #Layer 3 (output)\n",
        "    self.FC3 = nn.Sequential(\n",
        "      nn.Linear(hidden_dim, out_dim),\n",
        "      nn.BatchNorm1d(hidden_dim) )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x1 = self.FC1(x)\n",
        "    x2 = self.FC2(x1)\n",
        "    x_out = self.FC3(x2)\n",
        "    return x_out\n",
        "\n",
        "\n",
        "## PREDICTION MLP ## h(z)\n",
        "#has 2 FC layers, only BN and ReLU on hidden layer (explained why in SimSiam paper). \n",
        "# input: z = f(x) dim = 2048\n",
        "# output: p = h(z) dim = 2048\n",
        "class prediction_MLP(nn.Module):\n",
        "  def __init__(self, in_dim=2048, hidden_dim=512, out_dim=2048): # bottleneck structure\n",
        "    super().__init__()\n",
        "    #Layer 1\n",
        "    self.FC1 = nn.Sequential(\n",
        "      nn.Linear(in_dim, hidden_dim),\n",
        "      nn.BatchNorm1d(hidden_dim),\n",
        "      nn.ReLU(inplace=True) )\n",
        "    #Layer 2 (output)\n",
        "    self.FC2 = nn.Linear(hidden_dim, out_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x1 = self.FC1(x)\n",
        "    x_out = self.FC2(x1)\n",
        "    return x_out\n",
        "\n",
        "\n",
        "## SIMSIAM MODEL ##\n",
        "class SimSiam(nn.Module):\n",
        "  def __init__(self, backbone=FCN()): #take backbone as input\n",
        "    super().__init__()\n",
        "    self.name = SimSiam\n",
        "    self.backbone = backbone\n",
        "    self.projector = projection_MLP(backbone.output_dim) \n",
        "    \n",
        "    #Encoder; z = f(x)\n",
        "    self.encoder = nn.Sequential( \n",
        "      self.backbone,\n",
        "      self.projector )\n",
        "\n",
        "    #Predictor; p = h(z)\n",
        "    self.predictor = prediction_MLP()\n",
        "  \n",
        "  def forward(self, x1, x2):\n",
        "    #x1, x2: augmentations of x\n",
        "    f = self.encoder\n",
        "    h = self.predictor\n",
        "\n",
        "    z1, z2 = f(x1), f(x2)\n",
        "    p1, p2 = h(z1), h(z2)\n",
        "\n",
        "\n",
        "    #Symmetric loss\n",
        "    Loss = D(p1, z2)/2 + D(p2, z1)/2\n",
        "\n",
        "    return Loss\n",
        "\n",
        "###################\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X_e8bGxvc8o"
      },
      "source": [
        "##### TRAINING A SIMSIAM MODEL #####\n",
        "\n",
        "### Function to train model\n",
        "def train(model, optimizer, train_data, train_epochs, old_epoch=0, train_bs=512, lr=0.001, SaveName=\"SimSiam\"):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  train_loader = DataLoader(train_data, batch_size = train_bs, shuffle=True)\n",
        "  train_losses =[]\n",
        "\n",
        "  for e in range(old_epoch+1, old_epoch+train_epochs):\n",
        "    losses =[]\n",
        "    model.train()\n",
        "    for batch in enumerate(train_loader):\n",
        "      x1_batch = batch[1]['aug1']\n",
        "      x2_batch = batch[1]['aug2']\n",
        "\n",
        "      model.zero_grad()\n",
        "      loss = model.forward(x1_batch, x2_batch).mean()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      losses.append(loss.item())\n",
        "  \n",
        "    train_loss = np.mean(losses)\n",
        "    print(\"e: \",e,\" Avg Loss: \", train_loss)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    if (e in [ 5, 10, 15, 20, 30, 40, 50, 60, 70, 90]):\n",
        "      save_checkpoint(SaveName, model, e, optimizer, train_loss, lr, train_bs)\n",
        "\n",
        "  save_checkpoint(SaveName, model, e, optimizer, train_loss, lr, train_bs)\n",
        "\n",
        "### Function to save a checkpoint in Google Drive\n",
        "def save_checkpoint(name, model, epoch, optimizer, train_loss, lr, train_bs):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/{name}_{datetime.now().strftime('%d%m')}_ep:{epoch}.pth\"\n",
        "  checkpoint = {'epoch': epoch, \n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'train_loss': train_loss, \n",
        "              'lr': lr,\n",
        "              'train_bs' : train_bs}\n",
        "  torch.save(checkpoint, PATH) \n",
        "  \n",
        "### Function to load a previously saved checkpoint from Google Drive\n",
        "def load_checkpoint(name,date, e):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/{name}_{date}_ep:{e}.pth\"\n",
        "  checkpoint = torch.load(PATH)\n",
        "\n",
        "  epoch = checkpoint['epoch']\n",
        "  train_loss = checkpoint['train_loss']\n",
        "  #lr = checkpoint['lr']\n",
        "  #batch_size = checkpoint['batch_size']\n",
        "\n",
        "  #print(\"checkpoint:\", date,\" Epoch: \", epoch, \" Loss: \", train_loss, \"lr: \", lr, \"train_bs: \", train_bs)\n",
        "  print(\"checkpoint:\", date,\" Epoch: \", epoch, \" Loss: \", train_loss)\n",
        "  return checkpoint\n",
        "\n",
        "## Function to get a previously trained model from a checkpoint\n",
        "def load_trained_model(checkpoint):\n",
        "  epoch = checkpoint['epoch']\n",
        "  train_loss = checkpoint['train_loss']\n",
        "  #lr = checkpoint['lr']\n",
        "  #train_bs = checkpoint['train_bs']\n",
        "\n",
        "  train_bs = 512\n",
        "  base_lr = 0.05\n",
        "  lr = (base_lr*train_bs)/256\n",
        "  model = SimSiam().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  \n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  model.eval()\n",
        "\n",
        "  return model.to(device), optimizer, epoch\n",
        "\n",
        "\n",
        "###################"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaDBoUdqUbVY"
      },
      "source": [
        "##### LINEAR EVALUATION: training classifier on top of frozen model and evaluating #####\n",
        "\n",
        "## Function to calculate classification_accuracy given the test_set\n",
        "def evaluate_classifier(test_loader, backbone_model, classifier):\n",
        "  classifier.eval()\n",
        "  accuracies = []\n",
        "  for batch in enumerate(test_loader):\n",
        "    time_series_batch = batch[1]['time_series']\n",
        "    label_batch = batch[1]['label']\n",
        "\n",
        "    with torch.no_grad():\n",
        "      feature = backbone_model(time_series_batch)\n",
        "      y_hat = classifier(feature)\n",
        "      pred = torch.max(y_hat,1)[1]\n",
        "\n",
        "      #print(\"pred: \", pred)\n",
        "      #print(\"true: \", label_batch)\n",
        "      accuracy = accuracy_score(label_batch, pred) \n",
        "      accuracies.append(accuracy)\n",
        "  \n",
        "  Accuracy = np.mean(accuracies)\n",
        "  return Accuracy\n",
        "\n",
        "## function to calculate classification_accuracy of FCN model (to compare, just for now)\n",
        "def evaluate_FCN(test_loader, model):\n",
        "  model.eval()\n",
        "  accuracies = []\n",
        "  for batch in enumerate(test_loader):\n",
        "    time_series_batch = batch[1]['time_series']\n",
        "    label_batch = batch[1]['label']\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y_hat = model(time_series_batch)\n",
        "      pred = torch.max(y_hat,1)[1]\n",
        "\n",
        "      #print(\"pred: \", pred)\n",
        "      #print(\"true: \", label_batch)\n",
        "      accuracy = accuracy_score(label_batch, pred) \n",
        "      accuracies.append(accuracy)\n",
        "  \n",
        "  Accuracy = np.mean(accuracies)\n",
        "  return Accuracy\n",
        "\n",
        "\n",
        "## function to save checkpoint of linear classifier\n",
        "def save_checkpoint_classifier(SaveName, model, epoch, optimizer, loss_list, accuracy_list, lr, train_bs):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/classifier_{SaveName}_{datetime.now().strftime('%d%m')}_ep:{epoch}.pth\"\n",
        "  checkpoint = {'epoch': epoch, \n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss_list': loss_list, \n",
        "              'accuracy_list': accuracy_list,\n",
        "              'lr': lr,\n",
        "              'train_bs' : train_bs}\n",
        "  torch.save(checkpoint, PATH)\n",
        "\n",
        "## function to load checkpoint of linear classifier\n",
        "def load_checkpoint_classifier(name,date, e):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/classifier_{name}_{date}_ep:{e}.pth\"\n",
        "  checkpoint = torch.load(PATH)\n",
        "\n",
        "  epoch = checkpoint['epoch']\n",
        "  train_loss = checkpoint['loss_list'][-1]\n",
        "  max_accuracy = max(checkpoint['accuracy_list'])\n",
        "\n",
        "  print(\"checkpoint:\", date,\" Epoch: \", epoch, \" Max_accuracy: \",accuracy,\" Loss: \", train_loss)\n",
        "  return checkpoint\n",
        "\n",
        "\n",
        "## Function to do LINEAR EVALUATION of a model trained with SimSiam\n",
        "def Linear_evaluation(trained_model, classification_dataset, SaveName=None, epochs = 10, train_bs = 512, lr=None):\n",
        "  #trained model: a model prevoously trained with SimSiam\n",
        "  #classification_dataset: The dataset to train and evaluate the classification \n",
        "\n",
        "  dataset = classification_dataset.Datasets[0]\n",
        "  train_class, test_class = data_split(classification_dataset, train_size=0.5) #Split into a \n",
        "\n",
        "  if(lr==None):\n",
        "    base_lr = 0.05\n",
        "    lr = (base_lr*train_bs)/256\n",
        "\n",
        "  train_loader = DataLoader(train_class, batch_size = train_bs, shuffle=True)\n",
        "  test_loader = DataLoader(test_class, batch_size = train_bs, shuffle=True)\n",
        "\n",
        "  classes = test_class.classes\n",
        "\n",
        "  #Get the frozen backbone of the trained model\n",
        "  Backbone_model = trained_model.backbone\n",
        "  Backbone_model = Backbone_model.to(device)\n",
        "  Backbone_model = nn.DataParallel(Backbone_model)\n",
        "\n",
        "  #Linear classifier to train in top of frozen model\n",
        "  Linear = nn.Linear(128, classes).to(device)\n",
        "  optimizer_linear = torch.optim.Adam(Linear.parameters(), lr=lr)\n",
        "  accuracies_linear = []\n",
        "  losses_linear = []\n",
        "\n",
        "  #FCN model to compare accuracy (just for now..)\n",
        "  FCN_model = FCN(class_train=True, classes = classes).to(device)\n",
        "  optimizer_FCN = torch.optim.Adam(FCN_model.parameters(), lr=lr)\n",
        "  accuracies_FCN =[]\n",
        "  losses_FCN =[]\n",
        "\n",
        "  for e in range(epochs):\n",
        "    Backbone_model.eval()\n",
        "    Linear.train()\n",
        "    loss_list_linear = []\n",
        "    \n",
        "    FCN_model.train()\n",
        "    loss_list_FCN = []\n",
        "\n",
        "    for batch in enumerate(train_loader):\n",
        "      time_series_batch = batch[1]['time_series']\n",
        "      label_batch = batch[1]['label']\n",
        "\n",
        "      #Train linear classifier on top of backbone model\n",
        "      Linear.zero_grad()\n",
        "      with torch.no_grad():\n",
        "        feature = Backbone_model(time_series_batch.to(device))\n",
        "      pred = Linear(feature.to(device))\n",
        "      loss_linear = F.cross_entropy(pred, label_batch)\n",
        "      loss_linear.backward()\n",
        "      optimizer_linear.step()\n",
        "      loss_list_linear.append(loss_linear.item())\n",
        "      \n",
        "      #train FCN model\n",
        "      y_hat = FCN_model(time_series_batch.to(device))\n",
        "      loss_FCN = F.cross_entropy(y_hat, label_batch)\n",
        "      optimizer_FCN.zero_grad()\n",
        "      loss_FCN.backward()\n",
        "      optimizer_FCN.step()\n",
        "      loss_list_FCN.append(loss_FCN.item())\n",
        "\n",
        "    Loss_linear = np.mean(loss_list_linear)\n",
        "    losses_linear.append(Loss_linear)\n",
        "    accuracy_linear = evaluate_classifier(test_loader, Backbone_model, Linear)\n",
        "    accuracies_linear.append(accuracy_linear)\n",
        "\n",
        "    Loss_FCN = np.mean(loss_list_FCN)\n",
        "    losses_FCN.append(Loss_FCN)\n",
        "    accuracy_FCN = evaluate_FCN(test_loader, FCN_model)\n",
        "    accuracies_FCN.append(accuracy_FCN)\n",
        "\n",
        "    #print(\"e:  \", e, \"SimSiam : accuracy; \",accuracy_linear, \" loss; \",Loss_linear )\n",
        "    print(\"e:  \", e, \" dataset: \", dataset, \" : accuracy; \",accuracy_linear, \" loss; \",Loss_linear, \"   FCN: accuracy; \", accuracy_FCN, \" loss; \", Loss_FCN)\n",
        "\n",
        "  results = [dataset, \"linear eval\", e , train_bs, lr, accuracies_linear[-1], max(accuracies_linear), losses_linear[-1], max(accuracies_FCN)]\n",
        "\n",
        "  if (SaveName !=None):\n",
        "    save_checkpoint_classifier(SaveName, Linear, e, optimizer_linear, losses_linear, accuracies_linear, lr, train_bs) #Save checkpoint\n",
        "  \n",
        "  return results\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssP27LrqjHtJ"
      },
      "source": [
        "## Loading dataset\n",
        "\n",
        "#Dataset_small = Datasets = [\"ChlorineConcentration\"]\n",
        "#Small_dataset = Timeseries_Dataset(Dataset_small, transform = transforms.Compose( [TwoSegments(), ToTensor()] ))\n",
        "#train_small, test_small = data_split(Small_dataset)\n",
        "\n",
        "#Dataset_big = [\"Big_dataset\"]\n",
        "#Big_dataset = Timeseries_Dataset(Dataset_big, Drive=True, transform = transforms.Compose( [TwoSegments(), ToTensor()] ))\n",
        "#train_big, test_big = data_split(Big_dataset)\n",
        "\n",
        "#date=1904\n",
        "\n",
        "Big_dataset = [\"ChlorineConcentration\", \"Two_Patterns\", \"yoga\", \"uWaveGestureLibrary_X\", \"NonInvasiveFatalECG_Thorax1\", \"ECG5000\", \"FordA\", \"FordB\"]\n",
        "Save_BigDataset = \"BigDataset\"\n",
        "SaveName_BigDataset = \"BigDataset\"\n",
        "#BigDatasets_train = Timeseries_Dataset(Big_dataset, train=True, Save= None, transform = transforms.Compose( [TwoSegments(), ToTensor()] ))\n",
        "epoch_big = 30\n",
        "\n",
        "\n",
        "Two_datasets = [\"ChlorineConcentration\", \"Two_Patterns\"]\n",
        "Save_TwoDatasets = \"TwoDatasets_train\"\n",
        "SaveName_TwoDatasets = \"TwoDatasets\"\n",
        "#TwoDatasets_train = Timeseries_Dataset(Two_datasets, train=True, Save= Save, transform = transforms.Compose( [TwoSegments(), ToTensor()] ))\n",
        "epoch_two = 50\n",
        "\n",
        "\n",
        "\n",
        "date =2004\n",
        "epoch_big2 = 99\n",
        "SaveName_BigDataset2 = \"BigDataset2\"\n",
        "\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLD7R9lzvKDZ"
      },
      "source": [
        "### Training a SimSiam model\n",
        "SaveName_BigDataset2 = \"BigDataset2\"\n",
        "epochs = 100\n",
        "train_bs = 512\n",
        "\n",
        "base_lr = 0.05\n",
        "lr = (base_lr*train_bs)/256\n",
        "\n",
        "model = SimSiam().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train(model, optimizer, BigDatasets_train, epochs, old_epoch=0, train_bs=train_bs, lr=lr, SaveName=SaveName_BigDataset2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j_V9q17qekr"
      },
      "source": [
        "### Loading previously trained models\n",
        "\n",
        "#SaveName = \"SimSiam\"\n",
        "#checkpoint = load_checkpoint(SaveName, 1304, 99) #Loading the checkpoint that has trained 99 epochs with the train_small training set. \n",
        "#model_99, optimizer_99, epoch_99 = load_trained_model(checkpoint)\n",
        "\n",
        "#continue training previously trained model\n",
        "#train(model_99, optimizer_99, train_small, 2, old_epoch=epoch_99, train_bs=512, SaveName=\"SimSiam\")\n",
        "\n",
        "\n",
        "#checkpoint_two = load_checkpoint(SaveName_TwoDatasets, date, epoch_two)\n",
        "#model_two, optimizer_two, epoch_two = load_trained_model(checkpoint_two)\n",
        "\n",
        "#checkpoint_big = load_checkpoint(SaveName_BigDataset, date, epoch_big)\n",
        "#model_big, optimizer_big, epoch_big = load_trained_model(checkpoint_big)\n",
        "\n",
        "\n",
        "checkpoint_big2 = load_checkpoint(SaveName_BigDataset2, date, epoch_big2)\n",
        "model_big2, optimizer_big2, epoch_big2 = load_trained_model(checkpoint_big2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQzBUYtUpSMf"
      },
      "source": [
        "\n",
        "Big_dataset = [\"ChlorineConcentration\", \"Two_Patterns\", \"yoga\", \"uWaveGestureLibrary_X\", \"NonInvasiveFatalECG_Thorax1\", \"ECG5000\", \"FordA\", \"FordB\"]\n",
        "\n",
        "\n",
        "Results = []\n",
        "for dataset in Big_dataset:\n",
        "  classification_dataset = Timeseries_Dataset([dataset], train=False, Save=None, transform = transforms.Compose( [ ToTensor()] ))\n",
        "  train_class, test_class = data_split(classification_dataset, train_size=0.5)\n",
        "  SaveName = dataset\n",
        "\n",
        "  results = Linear_evaluation(model_big2, classification_dataset, SaveName=None, epochs = 20, train_bs = 40, lr=None)\n",
        "  results.append(SaveName_BigDataset)\n",
        "  results.append(date)\n",
        "  \n",
        "  Results.append(results)\n",
        "\n",
        "column_names = [\"Dataset\", \"Type\", \"epochs\", \"batch_size\", \"lr\", \"Accuracy\", \"Max Accuracy\", \"Loss\", \"FCN_accuracy\", \"trained model used\", \"date trained model\"]\n",
        "\n",
        "to_Excel(Results, column_names, \"Checking_LinearEval.xlsx\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}