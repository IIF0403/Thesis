{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset_class.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNgYnejSyTmVYoDVPNtpYJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IIF0403/Thesis/blob/main/Dataset_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19S8cs5RYp58"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import torch\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGCgMeDHklIM"
      },
      "source": [
        "\n",
        "### Function to load a UCR time_series dataset from my github ###\n",
        "def load_dataset(Dataset):\n",
        "  #Dataset: The name of the dataset to load. Example: \"ECG5000\"\n",
        "\n",
        "  #github URL\n",
        "  url_raw = 'https://raw.githubusercontent.com/IIF0403/Thesis/main/data/'\n",
        "  url_train = url_raw + Dataset+'/'+Dataset+'_TRAIN'\n",
        "  url_test = url_raw + Dataset+'/'+Dataset+'_TEST'\n",
        "\n",
        "  #Loading the data\n",
        "  data_train = pd.read_csv(url_train,header=None)\n",
        "  data_test = pd.read_csv(url_test, header=None)\n",
        "  data = pd.concat((data_train, data_test))\n",
        "\n",
        "  #Want all datasets to have classes as integers starting from 0 \n",
        "  Y = data.values[:,0]\n",
        "  classes = len(np.unique(Y))\n",
        "  Y_transformed = ( (Y-Y.min())/(Y.max()-Y.min()) )*(classes-1)\n",
        "  data[data.columns[0]] = Y_transformed\n",
        "\n",
        "  #Inserting the name of the dataset as a column (for later use, when several datasets will be combined)\n",
        "  data.insert(loc=0, column = \"Dataset\", value=Dataset) \n",
        "\n",
        "  #Inserting the length of the time series T as a column (for later use)\n",
        "  T = data.shape[1]-2 #The length of the time series\n",
        "  data.insert(loc=1, column = \"T\", value = T) \n",
        "\n",
        "  return data, classes\n",
        "\n",
        "\n",
        "### Function to make a big dataset out of several datasets loaded from github ###\n",
        "def make_big_dataset(Datasets):\n",
        "  #Datasets: A list of the name of the datasets to load. Example: [\"ECG5000\", \"FordA\", \"FordB\"]\n",
        "\n",
        "  #Loading first dataset\n",
        "  data, classes =  load_dataset(Datasets[0])\n",
        "  used_classes = classes #keeping track of the class-labels already used\n",
        "\n",
        "  #Loading each of the datasets in the list and combining them all togehter in a big dataset\n",
        "  for i in range(1, len(Datasets)):\n",
        "    #loading the i´th dataset\n",
        "    Dataset = Datasets[i]\n",
        "    dataset, classes= load_dataset(Dataset)\n",
        "\n",
        "    #Need to change the class-labels such that all the class-labels of the different datasets differ from eachother\n",
        "    labels = dataset.values[:,2]\n",
        "    transformed_labels = labels + used_classes \n",
        "    dataset[dataset.columns[2]] = transformed_labels\n",
        "    \n",
        "    used_classes += classes #keeping track of the class-labels already used\n",
        "\n",
        "    data = pd.concat((data, dataset)) #adding the new dataset to the big dataset\n",
        "  \n",
        "  return data, used_classes\n",
        "\n",
        "\n",
        "\n",
        "### Dataset class ###\n",
        "class Timeseries_Dataset(Dataset):\n",
        "  def __init__(self, Datasets, Drive=False, Save=None, transform=None):#, classes=None, T_values=None):\n",
        "    #Datasets: a list with the name of the datasets, can be a list og several or one dataset. If it is several datasets, they will be made into one big dataset\n",
        "    #Drive: True means loading the already saved dataset from google drive, false means loading data from github\n",
        "    #Save: name of new dataset, if we want to save the new dataset to google drive\n",
        "    #transform: a given transformation of the data\n",
        "\n",
        "    ##Loading the data\n",
        "    #if the data is saved in Google Drive, load the data from Drive\n",
        "    if (Drive == True):\n",
        "      Dataset = Datasets[0]\n",
        "      print(\"Loading '\",Dataset,\"' from Google Drive\")\n",
        "      drive.mount(\"/content/gdrive\")\n",
        "      data = pd.read_csv('/content/gdrive/My Drive/Datasets/'+Dataset+'.csv')\n",
        "      classes = len(np.unique(data.values[:,2]))\n",
        "    \n",
        "    #else load data from github\n",
        "    else:\n",
        "      #If \"Datasets\" contains several datasets then make a big dataset \n",
        "      if (len(Datasets)>1):\n",
        "        print(\"Loading and combining \", Datasets,\" from github\")\n",
        "        data, classes = make_big_dataset(Datasets)\n",
        "      \n",
        "      #If \"Datasets\" only contains one dataset, load the dataset from github\n",
        "      else:\n",
        "        Dataset = Datasets[0]\n",
        "        print(\"Loading '\",Dataset,\"' from github\")\n",
        "        data, classes = load_dataset(Dataset)\n",
        "    \n",
        "    #Save new dataset to Google drive as 'name' if Save = 'name' and not None\n",
        "    if (Save!=None): #Save the new dataset to google drive as Save\n",
        "      print(\"Saving new dataset to google drive\")\n",
        "      drive.mount(\"/content/gdrive\")\n",
        "      data.to_csv('/content/gdrive/My Drive/Datasets/'+Save+'.csv', index=False)\n",
        "\n",
        "    self.dataframe = data\n",
        "    self.transform = transform\n",
        "    self.classes = classes\n",
        "    self.Datasets = Datasets\n",
        "    \n",
        "  #defining the len(Dataset) function\n",
        "  def __len__(self): \n",
        "    return len(self.dataframe)\n",
        "\n",
        "  #defining the _getitem_ function which creates samples, such that when Dataset[i] is called; the i´th sample is returned\n",
        "  def __getitem__(self, idx): \n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    \n",
        "    #For one sample Dataset[i]:\n",
        "    dataframe = self.dataframe\n",
        "\n",
        "    label = dataframe.iloc[idx, 2] #retrieveing the label\n",
        "    dataset = dataframe.iloc[idx,0] #retrieveing the dataset-name\n",
        "    T = dataframe.iloc[idx,1] #retrieveing the timeseries-length\n",
        "    time_series_with_nan = dataframe.iloc[idx,3:].to_numpy() #retrieveing the timeseries (containing nan-values at the end)\n",
        "    time_series = time_series_with_nan[:T] #Removing nan_values at the end\n",
        "\n",
        "    sample = {'time_series': time_series, 'label': label, 'dataset': dataset, \"T\": T} #a sample is one timeseries with it's corresponding label (and som xtra information)\n",
        "\n",
        "    if self.transform: #transform sample\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hErsah9d9Vba",
        "outputId": "fce54626-bdef-4053-99c5-5f937c601a23"
      },
      "source": [
        "##Testing the Dataset class with one dataset\n",
        "Datdat1 = [\"ChlorineConcentration\"]\n",
        "test_1_dataset = Timeseries_Dataset(Datdat1)\n",
        "print(\"Datasets: \", test_1_dataset.Datasets)\n",
        "print(\"classes: \", test_1_dataset.classes)\n",
        "\n",
        "##Testing the Dataset class with two datasets\n",
        "Datdat2 = [\"ChlorineConcentration\", \"ECG5000\"]\n",
        "test_2_dataset = Timeseries_Dataset(Datdat2)\n",
        "print(\"Datasets: \", test_2_dataset.Datasets)\n",
        "print(\"classes: \", test_2_dataset.classes)\n",
        "\n",
        "data_1 = test_1_dataset.dataframe\n",
        "data_2 = test_2_dataset.dataframe\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading ' ChlorineConcentration ' from github\n",
            "Datasets:  ['ChlorineConcentration']\n",
            "classes:  3\n",
            "Loading and combining  ['ChlorineConcentration', 'ECG5000']  from github\n",
            "Datasets:  ['ChlorineConcentration', 'ECG5000']\n",
            "classes:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNhbjQ3PY2Xp",
        "outputId": "51bd0787-8fbb-45d4-e162-eb5456280e5c"
      },
      "source": [
        "##Testing dataset class with many Datasets to make a big dataset and save it in google drive as \"Big_dataset\" for later use\n",
        "#Datasets_UCR = [\"ChlorineConcentration\", \"ECG5000\", \"ElectricDevices\", \"FordA\", \"FordB\", \"NonInvasiveFatalECG_Thorax1\", \"PhalangesOutlinesCorrect\", \"Two_Patterns\", \"uWaveGestureLibrary_X\", \"yoga\"]\n",
        "#Big_dataset = Timeseries_Dataset(Datasets_UCR, Save=\"Big_dataset\") \n",
        "\n",
        "\n",
        "##Testing the dataset class with the big dataset already saved in google drive as \"Big_dataset\"\n",
        "Dataset_big = [\"Big_dataset\"]\n",
        "Big_dataset = Timeseries_Dataset(Dataset_big, Drive=True)\n",
        "\n",
        "print(\"Dataset: \", Big_dataset.Datasets)\n",
        "print(\"classes: \", Big_dataset.classes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading ' Big_dataset ' from Google Drive\n",
            "Mounted at /content/gdrive\n",
            "Dataset:  ['Big_dataset']\n",
            "classes:  77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbCGclNS7lXt",
        "outputId": "f9745e8f-3db4-4907-c613-7388ddc36d49"
      },
      "source": [
        "#Cecking a random sample from the big dataset\n",
        "sample = Big_dataset[5000]\n",
        "\n",
        "print(\"Dataset: \", sample['dataset'])\n",
        "print(\"label: \", sample['label'])\n",
        "print(\"time_series\", sample['time_series'] )\n",
        "print(\"length\", len(sample['time_series']),\" = \", sample['T'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset:  ECG5000\n",
            "label:  3.0\n",
            "time_series [-0.46884 -1.7882 -3.1189 -3.9938 -4.4052 -4.0492 -3.442 -2.3566 -1.3525\n",
            " -1.1544 -0.62167 -0.16988 -0.25256 -0.23814 -0.21724 -0.24842 -0.31426\n",
            " -0.25116 -0.28998 -0.20823000000000005 -0.30376 -0.28241 -0.17132 -0.2781\n",
            " -0.39058 -0.4749 -0.34214 -0.25839 -0.2532 -0.16116 -0.037112 -0.2593\n",
            " -0.27182 -0.21782 -0.24647 -0.24586 -0.36634 -0.18614 -0.12941 -0.17467\n",
            " -0.21508 -0.052802 -0.065706 -0.090324 -0.0058875 -0.030413 0.055523\n",
            " 0.090912 0.19572 0.14973 -0.018653 0.14499 0.093002 0.30346 0.21618\n",
            " 0.15542 0.06979099999999999 0.13327 0.1438 0.025846 0.092808 -0.04918\n",
            " -0.066777 0.0066759 0.043886 0.095751 -0.025595 0.055351 0.14149 0.16483\n",
            " 0.20983 0.25157 0.1959 0.47397 0.51012 0.32099 0.43829 0.5027\n",
            " 0.4114600000000001 0.42173 0.48473 0.35172 0.30354000000000003 0.37531\n",
            " 0.50374 0.46707 0.38394 0.3919 0.37201 0.5624600000000001 0.5508 0.38089\n",
            " 0.34915 0.43562 0.5854 0.86035 0.89151 1.0449 1.2377 1.4923 1.8545 2.2274\n",
            " 2.3473 2.1307 1.9825 1.6621 1.5202 1.2843 1.0498 0.62756 0.28493 0.016382\n",
            " -0.14737999999999998 -0.16710999999999998 -0.39719 -0.40289 -0.14702\n",
            " -0.22847 -0.15989 -0.21686 -0.13518 -0.086749 -0.20389 -0.19243 -0.19733\n",
            " -0.21427 -0.10264 -0.11376 0.11581 0.6773100000000001 1.3118 1.4337\n",
            " 1.1456 0.8238 0.2 -0.27304 -0.63827 -0.95418 -0.5523899999999999\n",
            " -0.6923600000000001]\n",
            "length 140  =  140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjCbxOYqZ97s"
      },
      "source": [
        "\n",
        "### Transformation class; segment timeseries into two augmentations\n",
        "class TwoSegments(object):\n",
        "  def __init__(self, horizon=0.3, window_gap=1, random_startpos = False, random_horizon=False, random_window_gap=False):\n",
        "    #horizon: horizon*T = window_length; the length of the two augmentations\n",
        "    #window_gap: the gap bewteen the two augmentations\n",
        "    #random_startpos: True means that the first augmentation starts at a random position\n",
        "    #random_horizon: True means that a random horizon is chosen\n",
        "    #random_window_gap: True means that a random window_gap is chosen\n",
        "\n",
        "    self.horizon = horizon\n",
        "    self.window_gap = window_gap\n",
        "    self.random_startpos = random_startpos\n",
        "    self.random_horizon = random_horizon\n",
        "    self.random_window_gap = random_window_gap\n",
        "        \n",
        "  def __call__(self, sample):\n",
        "\n",
        "    dataset = sample['dataset']\n",
        "    time_series = sample['time_series']\n",
        "    T = sample['T']\n",
        "    label = sample['label']\n",
        "\n",
        "    #horizon\n",
        "    if (self.random_horizon==True):\n",
        "      possible_horizons = [0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "      horizon = random.choice(possible_horizons) #draw a random horizon\n",
        "    else:\n",
        "      horizon = self.horizon\n",
        "\n",
        "    #window gap\n",
        "    if (self.random_window_gap==True):\n",
        "      possible_window_gaps =[0,1,2,3,4,5,6,7,8,9,10]\n",
        "      window_gap = random.choice(possible_window_gaps) #draw random window_gap\n",
        "    else:\n",
        "      window_gap = self.window_gap\n",
        "\n",
        "    window_length = int(horizon*T) #length of each augmentation\n",
        "\n",
        "    #finding start-position of the first augmentation\n",
        "    if (self.random_startpos == True): #if random start_position\n",
        "      max_possible_startposition = T-(2*window_length+window_gap) #the maximal start-position of the first augmentation\n",
        "      possible_startpossisions = [i for i in range(max_possible_startposition_aug1)] #The possible start positions of the first augmentation\n",
        "      start_pos = random.choice(possible_startpossisions) #draw a random startposition\n",
        "    else:\n",
        "      start_pos = 0 \n",
        "\n",
        "    #make the two augmentations of the timeseries\n",
        "    augmentation_1 = time_series[start_pos : (start_pos+window_length)]\n",
        "    augmentation_2 = time_series[(start_pos+window_length+window_gap) : (start_pos+window_length+window_gap)+window_length]\n",
        "\n",
        "    #create a new sample with the two augmentations\n",
        "    new_sample = {'aug1': augmentation_1, 'aug2': augmentation_2, 'label': label, 'dataset': dataset, \"T\": T}\n",
        "\n",
        "    return new_sample\n",
        "\n",
        "### Transformation class; convert into Tensor-data for PyTorch-use\n",
        "class ToTensor(object):\n",
        "  def __call__(self, sample):\n",
        "    dataset = sample['dataset']\n",
        "    T = sample['T']\n",
        "    label = sample['label']\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if (len(sample)== 5):\n",
        "      aug1 = sample['aug1'].astype(float)\n",
        "      aug2 = sample['aug2'].astype(float)\n",
        "      label = label.astype(float)\n",
        "\n",
        "      aug1_tensor = torch.tensor(aug1, dtype=torch.float32, device=device)\n",
        "      aug2_tensor = torch.tensor(aug2, dtype=torch.float32, device=device)\n",
        "      label_tensor = torch.tensor(label, dtype=torch.long, device=device)\n",
        "\n",
        "      torch_sample = {'aug1': aug1_tensor, 'aug2': aug2_tensor, 'label': label_tensor, 'dataset': dataset, \"T\": T}\n",
        "\n",
        "    else:\n",
        "      time_series = sample['timeseries'].astype(float)\n",
        "      label = label.astype(float)\n",
        "\n",
        "      time_series_tensor = torch.tensor(time_series, dtype=torch.float32, device=device)\n",
        "      label_tensor = torch.tensor(label, dtype=torch.long, device=device)\n",
        "\n",
        "      torch_sample = {'time_series': time_series_tensor, 'label': label_tensor, 'dataset': dataset, \"T\": T}\n",
        "\n",
        "    return torch_sample\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLfqcekPx3wU",
        "outputId": "64fb68e1-0c71-4388-d425-34d1ad616d44"
      },
      "source": [
        "## Testing dataset with transformation\n",
        "\n",
        "##Testing the Dataset class with one dataset with transformation\n",
        "Datdat1 = [\"ChlorineConcentration\"]\n",
        "\n",
        "#transformed_dataset = Timeseries_Dataset(Datdat1, transform = transform.Compose( [TwoSegments(), ToTensor()] )\n",
        "\n",
        "#dat1 = Timeseries_Dataset(Datdat1) #normal dataset\n",
        "trans_dat1 = Timeseries_Dataset(Datdat1, transform = TwoSegments() ) #transformed dataset\n",
        "tens_dat1 = Timeseries_Dataset(Datdat1, transform = transforms.Compose( [TwoSegments(), ToTensor()] ) )\n",
        "\n",
        "print(\"Datasets: \", trans_dat1.Datasets)\n",
        "print(\"classes: \", trans_dat1.classes)\n",
        "\n",
        "sample = dat1[0]\n",
        "sample_trans = trans_dat1[0]\n",
        "sample_tens = tens_dat1[0]\n",
        "\n",
        "#print(\"time series: \", sample['time_series'])\n",
        "print(\"aug1: \", sample_trans['aug1'])\n",
        "print(\"aug1 tensor: \", sample_tens['aug1'].size())\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading ' ChlorineConcentration ' from github\n",
            "Loading ' ChlorineConcentration ' from github\n",
            "Datasets:  ['ChlorineConcentration']\n",
            "classes:  3\n",
            "aug1:  [2.6173 3.2310000000000003 2.8508 2.7515 2.3457 2.2746 1.9898 1.849 1.4533\n",
            " 1.3171 1.1623 0.9791 0.76748 -0.27063000000000004 1.4944 1.4457\n",
            " 1.2830000000000001 1.1554 0.73141 0.53898 0.26045 0.058329 -0.30429\n",
            " -0.60378 -0.92625 -1.0126 1.8559 1.5001 1.3893 0.9980899999999999 0.68913\n",
            " 0.35189000000000004 0.078764 -0.41918 -0.74724 -1.0089 -1.0009 -1.0751\n",
            " 0.9516600000000001 1.4072 0.83868 0.91775 -0.18871 -0.76323 -0.57789\n",
            " -1.0485 -0.68799 -0.94221 -1.0463]\n",
            "aug1 tensor:  torch.Size([49])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FpDlEFr3uGX"
      },
      "source": [
        "#Testing DataLoader\n",
        "data_loader1 = DataLoader(tens_dat1, batch_size=4, shuffle=False, num_workers =0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y7fgiQd36W9"
      },
      "source": [
        "for samples in enumerate(data_loader1):\n",
        "  print(samples[1]['aug1'].size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdQgQ-k0Zinw"
      },
      "source": [
        "\n",
        "#### Overføre laget dataset til drive\n",
        "#drive.mount(\"/content/gdrive\")\n",
        "#big_dataset.to_csv('/content/gdrive/My Drive/Datasets/big_dataset.csv', index=False)\n",
        "\n",
        "#### Laste ned csv fil\n",
        "#files.download(\"big_dataset.csv\")\n",
        "\n",
        "#### Åpne dataset fra drive\n",
        "#drive.mount(\"/content/gdrive\")\n",
        "#the_data = pd.read_csv('/content/gdrive/My Drive/Datasets/big_dataset.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}