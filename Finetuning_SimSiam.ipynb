{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Finetuning_SimSiam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPAXacmY1pT4hSYXocNMB7q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IIF0403/Thesis/blob/main/Finetuning_SimSiam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd07YtQtkrm3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "from google.colab import drive\n",
        "from torch.nn.utils.rnn import pack_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "import random\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGHV1EGOY_Zi"
      },
      "source": [
        "#Importing Dataset class from other ipynb file \"SimSiam_training\"\n",
        "!pip install import_ipynb\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "#!ls\n",
        "import import_ipynb\n",
        "from SimSiam_training import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaDBoUdqUbVY"
      },
      "source": [
        "### Some helping functions ###\n",
        "\n",
        "#Function to calculate Accuracy score\n",
        "def Accuracy_score(labels, preds):\n",
        "  if len(labels)!=len(preds):\n",
        "    print(\"sizes does not match\")\n",
        "  else:\n",
        "    total=0\n",
        "    correct=0\n",
        "    for i in range(len(labels)):\n",
        "      if labels[i]==preds[i]:\n",
        "        correct+=1\n",
        "      total+=1\n",
        "    return (correct/total)\n",
        "\n",
        "## Function to calculate classification_accuracy of a classifier given a frozen backbone model and the test set\n",
        "def evaluate_classifier(test_loader, backbone_model, classifier):\n",
        "  classifier.eval()\n",
        "  accuracies = []\n",
        "  for batch in enumerate(test_loader):\n",
        "    time_series_batch = batch[1]['time_series']\n",
        "    label_batch = batch[1]['label']\n",
        "\n",
        "    with torch.no_grad():\n",
        "      feature = backbone_model(time_series_batch)\n",
        "      y_hat = classifier(feature)\n",
        "      pred = torch.max(y_hat,1)[1]\n",
        "      #print(\"pred: \", pred)\n",
        "      #print(\"true: \", label_batch)\n",
        "      accuracy = Accuracy_score(label_batch, pred) \n",
        "      accuracies.append(accuracy)\n",
        "  \n",
        "  Accuracy = np.mean(accuracies)\n",
        "  return Accuracy\n",
        "\n",
        "## Function to calculate classification_accuracy of a model (compplete model) given the test set\n",
        "def evaluate_model(test_loader, model):\n",
        "  model.eval()\n",
        "  accuracies = []\n",
        "  for batch in enumerate(test_loader):\n",
        "    time_series_batch = batch[1]['time_series']\n",
        "    label_batch = batch[1]['label']\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y_hat = model(time_series_batch)\n",
        "      pred = torch.max(y_hat,1)[1]\n",
        "      #print(\"pred: \", pred)\n",
        "      #print(\"true: \", label_batch)\n",
        "      accuracy = Accuracy_score(label_batch, pred) \n",
        "      accuracies.append(accuracy)\n",
        "  \n",
        "  Accuracy = np.mean(accuracies)\n",
        "  return Accuracy\n",
        "\n",
        "\n",
        "## function to save checkpoint of linear classifier\n",
        "def save_checkpoint_classifier(SaveName, model, epoch, optimizer, loss_list, accuracy_list, lr, train_bs):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/classifier_{SaveName}.pth\"\n",
        "  checkpoint = {'epoch': epoch, \n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss_list': loss_list, \n",
        "              'accuracy_list': accuracy_list,\n",
        "              'lr': lr,\n",
        "              'train_bs' : train_bs}\n",
        "  torch.save(checkpoint, PATH)\n",
        "\n",
        "\n",
        "## function to load checkpoint of linear classifier\n",
        "def load_checkpoint_classifier(name):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/classifier_{name}.pth\"\n",
        "  checkpoint = torch.load(PATH)\n",
        "\n",
        "  epoch = checkpoint['epoch']\n",
        "  train_loss = checkpoint['loss_list'][-1]\n",
        "  max_accuracy = max(checkpoint['accuracy_list'])\n",
        "\n",
        "  print(\"checkpoint loaded:\",\" Accuracy: \",max_accuracy,\" Loss: \", train_loss)\n",
        "  return checkpoint\n",
        "\n",
        "def load_trained_classifier(checkpoint, classes):\n",
        "  #epoch = checkpoint['epoch']\n",
        "\n",
        "  lr = checkpoint['lr']\n",
        "\n",
        "  Lin = nn.Linear(128, classes).to(device)\n",
        "  optim = torch.optim.Adam(Lin.parameters(), lr=lr)\n",
        "\n",
        "  Lin.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "  Lin.eval()\n",
        "\n",
        "  return Lin.to(device), optim\n",
        "\n",
        "#Function to print value rounded to 4 desimals \n",
        "def rounded(value):\n",
        "  format = \"{:.4f}\".format(value)\n",
        "  float_value = float(format)\n",
        "  return float_value\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Function to plot Accuracy or Loss over epochs\n",
        "def plot_progress(res, title, save=True):\n",
        "  N = len(res)\n",
        "  epochs = [i for i in range(N)]\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, res)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.title(title)\n",
        "\n",
        "  if (save==True):\n",
        "    plt.savefig(title+\".png\")\n",
        "    files.download(title+\".png\")\n",
        "  plt.show()\n",
        "\n",
        "#### FineTune network model ####\n",
        "#Combines the Backbone model and the Linear to a complete model for finetuning\n",
        "class FineTuneModel(nn.Module):\n",
        "  def __init__(self, Backbone, Linear):\n",
        "    super(FineTuneModel, self).__init__()\n",
        "    self.Backbone = Backbone\n",
        "    self.Linear = Linear\n",
        "  \n",
        "  def forward(self, x):\n",
        "    feature = self.Backbone(x)\n",
        "    pred = self.Linear(feature)\n",
        "    return pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGIwlmgYtlwU"
      },
      "source": [
        "\n",
        "## Function to do FineTuning \n",
        "def FineTuning(trained_model, train_set, test_set, Linear = None, SaveName=None, epochs_Linear = 10, epochs_finetune=10, train_bs = 40, lr_linear=0.001, lr_finetune=0.001, backbone = \"FCN\"):\n",
        "  #trained model: a model previously trained with SimSiam\n",
        "  #train_set, test_set: The datasets to train and evaluate the classification \n",
        "  \n",
        "  dataset = train_set.Datasets[0]\n",
        "  classes = test_set.classes\n",
        "\n",
        "  Results = []\n",
        "\n",
        "  train_loader = DataLoader(train_set, batch_size = train_bs, shuffle=True)\n",
        "  test_loader = DataLoader(test_set, batch_size = train_bs, shuffle=True)\n",
        "  \n",
        "  #Get the frozen backbone of the trained model\n",
        "  Backbone_model = trained_model.backbone\n",
        "  Backbone_model = Backbone_model.to(device)\n",
        "  Backbone_model = nn.DataParallel(Backbone_model)\n",
        "\n",
        "  accuracies_linear = []\n",
        "  losses_linear = []\n",
        "\n",
        "  if (Linear==None):\n",
        "    #############################################################\n",
        "    #If trained Linear layer is not given \n",
        "    #Train Linear layer on frozen Backbone\n",
        "    lr_linear = lr_linear\n",
        "    Linear = nn.Linear(128, classes).to(device)\n",
        "    optimizer_linear = torch.optim.Adam(Linear.parameters(), lr=lr_linear)\n",
        "  \n",
        "    print(\"Training Linear Layer\")\n",
        "    for e in range(epochs_Linear):\n",
        "      Backbone_model.eval()\n",
        "      Linear.train()\n",
        "      loss_list_linear = []\n",
        "\n",
        "      for batch in enumerate(train_loader):\n",
        "        time_series_batch = batch[1]['time_series']\n",
        "        label_batch = batch[1]['label']\n",
        "\n",
        "        #Train linear classifier on top of backbone model\n",
        "        Linear.zero_grad()\n",
        "        with torch.no_grad():\n",
        "          feature = Backbone_model(time_series_batch.to(device))\n",
        "        pred = Linear(feature.to(device))\n",
        "        loss_linear = F.cross_entropy(pred, label_batch)\n",
        "        loss_linear.backward()\n",
        "        optimizer_linear.step()\n",
        "        loss_list_linear.append(loss_linear.item())\n",
        "        \n",
        "      Loss_linear = np.mean(loss_list_linear)\n",
        "      losses_linear.append(Loss_linear)\n",
        "      accuracy_linear = evaluate_classifier(test_loader, Backbone_model, Linear)\n",
        "      accuracies_linear.append(accuracy_linear)\n",
        "\n",
        "      if (e in [50,100,200, 299]):\n",
        "        print(\"e:  \", e, \" dataset: \", dataset,\"  SimSiam:   accuracy; \", rounded(accuracy_linear),\" max Acc: \",max(accuracies_linear) ,\" loss; \",rounded(Loss_linear) )\n",
        "\n",
        "    results_linear = [dataset, \"LinearEval\", e , train_bs, lr_linear, accuracies_linear[-1], max(accuracies_linear), losses_linear[-1], -1, -1]\n",
        "    Results.append(results_linear)\n",
        "\n",
        "    #############################################################\n",
        "  \n",
        "  else: #If Linear already trained is given, use this\n",
        "    Linear = nn.DataParallel(Linear).to(device)\n",
        "\n",
        "  ############### Unfreeze and train the whole network ##################\n",
        "  print(\"Fine tuning the whole network\")\n",
        "\n",
        "  #Get a copy of the trained frozen backbone and the trained Linear classifier\n",
        "  Backbone_finetune = nn.DataParallel(Backbone_model).to(device)\n",
        "  Linear_finetune = nn.DataParallel(Linear).to(device)\n",
        "\n",
        "  FineTune_model = FineTuneModel(Backbone_finetune, Linear_finetune) #Combine Backbone and Linear to a single model for finetuning\n",
        "\n",
        "  lr_FineTune =lr_finetune\n",
        "  optimizer_FineTune = torch.optim.Adam(FineTune_model.parameters(), lr=lr_FineTune)\n",
        "\n",
        "  losses_FineTune = []\n",
        "  accuracies_FineTune = []\n",
        "\n",
        "  for e in range(epochs_finetune):\n",
        "    FineTune_model.train()\n",
        "    loss_list_Finetune = []\n",
        "\n",
        "    for batch in enumerate(train_loader):\n",
        "      time_series_batch = batch[1]['time_series']\n",
        "      label_batch = batch[1]['label']\n",
        "\n",
        "      #Train the FineTune model\n",
        "      FineTune_model.zero_grad()\n",
        "      pred = FineTune_model(time_series_batch.to(device))\n",
        "\n",
        "      loss_FineTune = F.cross_entropy(pred, label_batch)\n",
        "      loss_FineTune.backward()\n",
        "      optimizer_FineTune.step()\n",
        "      loss_list_Finetune.append(loss_FineTune.item())\n",
        "\n",
        "\n",
        "    Loss_FineTune = np.mean(loss_list_Finetune)\n",
        "    losses_FineTune.append(Loss_FineTune)\n",
        "    accuracy_FineTune = evaluate_model(test_loader, FineTune_model)\n",
        "    accuracies_FineTune.append(accuracy_FineTune)\n",
        "\n",
        "    print(\"e:  \", e, \" dataset: \", dataset,\"  SimSiam:   accuracy; \",rounded(accuracy_FineTune),\" max Acc: \",max(accuracies_FineTune), \" loss; \",rounded(Loss_FineTune))\n",
        "\n",
        "  results_finetune = [dataset, \"FineTuning\", e , train_bs, lr_FineTune, accuracies_FineTune[-1], max(accuracies_FineTune), losses_FineTune[-1], .1, -1]\n",
        "  Results.append(results_finetune)\n",
        "  #column_names = [\"Dataset\", \"Type\", \"epochs\", \"batch_size\", \"lr\", \"last_Acc\", \"Model Accuracy\", \"Loss Model\", \"Random model accuracy\", \"Loss Random\", \"trained model used\", \"date model\", \"labeled size\"]\n",
        "\n",
        "  #Plotting Accuracy and Loss over epochs\n",
        "  plot_progress(accuracies_FineTune, \"Accuracy_FineTune_\"+dataset, save=False)\n",
        "  plot_progress(losses_FineTune, \"Loss_FineTune_\"+dataset, save=False)\n",
        "\n",
        "  if (SaveName !=None):\n",
        "    save_checkpoint_classifier(SaveName, FineTuneModel, e, optimizer_FineTune, losses_FineTune, accuracies_FineTune, lr_FineTune, train_bs) #Save checkpoint\n",
        "\n",
        "  return Results\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jeJph1lYqZxJ"
      },
      "source": [
        "##### FineTuning on each dataset of the big dataset #####\n",
        "\n",
        "### The datasets\n",
        "Datasets = [\"ChlorineConcentration\", \"ECG5000\", \"ElectricDevices\", \"FordA\", \"FordB\",\"Two_Patterns\", \"wafer\", \"yoga\"]\n",
        "\n",
        "### Learning rates found using Optuna \n",
        "lr_FCN_linear = [ 0.0097,0.0278 ,0.0623 ,0.0689 , 0.0222, 0.0854, 0.0623 ,0.0544]  \n",
        "lr_ResNet_linear = [ 0.0072, 0.0191, 0.0046, 0.0233, 0.0002,0.0370, 0.0265,0.0234]\n",
        "\n",
        "Lr_FCN_finetune = [0.0263,0.0461,0.0059,0.0376,0.0032,0.0062,0.0305,0.0057]\n",
        "Lr_ResNet_finetune = [0.0095, 0.0027,0.0037,0.0011,0.0052,0.0004,0.0153,0.0094]\n",
        "\n",
        "\n",
        "### For finetuning of the new datasets\n",
        "#Datasets = [\"TwoLeadECG\", \"SmallKitchenAppliances\", \"MoteStrain\", \"CinC_ECG_torso\"]\n",
        "#Lr_FCN_finetune = [0.0536, 0.0105, 0.0032,0.00176] #Found with Optuna\n",
        "#Lr_ResNet_finetune = [0.00446, 0.04377,0.00412,0.00344]\n",
        "\n",
        "\n",
        "Backbones = [\"FCN\", \"ResNet\"]\n",
        "\n",
        "batch_size = 512\n",
        "\n",
        "labeled_sizes = [0.2, 0.1, 0.05]\n",
        "\n",
        "rounds = 1\n",
        "\n",
        "epochs_Linear = 300 \n",
        "epochs_FineTune = 300 \n",
        "\n",
        "Results = []\n",
        "\n",
        "for backbone in Backbones:\n",
        "  if (backbone==\"FCN\"):\n",
        "    #Information to load the saved SimSiam_FCN model from Google drive\n",
        "    SaveName = \"Window38\"\n",
        "    date = 2704\n",
        "    epoch = 99\n",
        "\n",
        "    #Set the Learning rate list as the optimal learning rates previously found by Optuna\n",
        "    Lr_linear = Lr_FCN_finetune\n",
        "    Lr_Finetune = Lr_FCN_finetune\n",
        "\n",
        "  else:\n",
        "    #Information to load the saved SimSiam_ResNet model from Google drive\n",
        "    SaveName = \"ResNet_W38\"\n",
        "    date = 2904\n",
        "    epoch = 99\n",
        "\n",
        "    #Set the Learning rate list as the optimal learning rates previously found by Optuna\n",
        "    Lr_linear = Lr_ResNet_finetune\n",
        "    Lr_Finetune = Lr_ResNet_finetune\n",
        "\n",
        "\n",
        "  ##Loading the pre-trained SimSiam model\n",
        "  checkpoint = load_checkpoint(SaveName, date, epoch)\n",
        "  trained_model, optimizer_model, epoch_model, lr_model = load_trained_model(checkpoint, backbone=backbone)\n",
        "\n",
        "  for i in range(len(Datasets)):\n",
        "    dataset = Datasets[i]\n",
        "    lr_linear = Lr_linear[i]\n",
        "    lr_finetune = Lr_Finetune[i]\n",
        "\n",
        "    for labeled_size in labeled_sizes:\n",
        "      Train_set = Timeseries_Dataset([dataset], train=True, Save=None, transform = transforms.Compose( [ ToTensor()] )) #the training set\n",
        "      N_train = len(Train_set)\n",
        "\n",
        "      Train_set.shuffle()\n",
        "      Train_set_labeled = Train_set[:int(N_train*labeled_size)] #the labeled set\n",
        "\n",
        "      Test_set = Timeseries_Dataset([dataset], train=False, Save=None, transform = transforms.Compose( [ ToTensor()] )) #the test set\n",
        "\n",
        "      for round in range(rounds):\n",
        "        Train_set_labeled.shuffle()\n",
        "        Test_set.shuffle()\n",
        "        \n",
        "        Save_checkpoint_name = None\n",
        "\n",
        "        Linear_trained = None\n",
        "\n",
        "        Results_1 = FineTuning(trained_model, Train_set_labeled, Test_set, Linear = Linear_trained, SaveName=Save_checkpoint_name, epochs_Linear = epochs_Linear, epochs_finetune=epochs_FineTune, train_bs = batch_size, lr_linear=lr_linear, lr_finetune=lr_finetune, backbone = backbone)\n",
        "        \n",
        "        for res in Results_1:\n",
        "          result = res\n",
        "          result.append(SaveName)\n",
        "          result.append(date)\n",
        "          result.append(labeled_size)\n",
        "\n",
        "          Results.append(result)\n",
        "  \n",
        "    column_names = [\"Dataset\", \"Type\", \"epochs\", \"batch_size\", \"lr\", \"last_Acc\", \"Model Accuracy\", \"Loss Model\", \"Random model accuracy\", \"Loss Random\", \"trained model used\", \"date model\", \"labeled size\"]    \n",
        "    to_Excel(Results, column_names, \"FineTune_\"+SaveName+\"_\"+dataset+\".xlsx\")\n",
        "\n",
        "\n",
        "column_names = [\"Dataset\", \"Type\", \"epochs\", \"batch_size\", \"lr\", \"last_Acc\", \"Model Accuracy\", \"Loss Model\", \"Random model accuracy\", \"Loss Random\", \"trained model used\", \"date model\", \"labeled size\"]\n",
        "\n",
        "name = \"_FineTune_\"\n",
        "to_Excel(Results, column_names, name+\".xlsx\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}