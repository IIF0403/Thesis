{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_evaluation(u17).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyFIOkUUI7bpY6DrCTK+ze",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IIF0403/Thesis/blob/main/Linear_evaluation(u17).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Kbtvs3gINM"
      },
      "source": [
        "#Check GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd07YtQtkrm3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "from google.colab import drive\n",
        "from torch.nn.utils.rnn import pack_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "import random\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGHV1EGOY_Zi"
      },
      "source": [
        "#Importing Dataset class from other ipynb file\n",
        "!pip install import_ipynb\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "#!ls\n",
        "import import_ipynb\n",
        "from SimSiam_training import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaDBoUdqUbVY"
      },
      "source": [
        "##### LINEAR EVALUATION: training classifier on top of frozen model and evaluating #####\n",
        "def Accuracy_score(labels, preds):\n",
        "  if len(labels)!=len(preds):\n",
        "    print(\"sizes does not match\")\n",
        "  else:\n",
        "    total=0\n",
        "    correct=0\n",
        "    for i in range(len(labels)):\n",
        "      if labels[i]==preds[i]:\n",
        "        correct+=1\n",
        "      total+=1\n",
        "    return (correct/total)\n",
        "\n",
        "## Function to calculate classification_accuracy given the test_set\n",
        "def evaluate_classifier(test_loader, backbone_model, classifier):\n",
        "  classifier.eval()\n",
        "  accuracies = []\n",
        "  for batch in enumerate(test_loader):\n",
        "    time_series_batch = batch[1]['time_series']\n",
        "    label_batch = batch[1]['label']\n",
        "\n",
        "    with torch.no_grad():\n",
        "      feature = backbone_model(time_series_batch)\n",
        "      y_hat = classifier(feature)\n",
        "      pred = torch.max(y_hat,1)[1]\n",
        "      #print(\"pred: \", pred)\n",
        "      #print(\"true: \", label_batch)\n",
        "      accuracy = Accuracy_score(label_batch, pred) \n",
        "      accuracies.append(accuracy)\n",
        "  \n",
        "  Accuracy = np.mean(accuracies)\n",
        "  return Accuracy\n",
        "\n",
        "\n",
        "## function to save checkpoint of linear classifier\n",
        "def save_checkpoint_classifier(SaveName, model, epoch, optimizer, loss_list, accuracy_list, lr, train_bs):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/classifier_{SaveName}_{datetime.now().strftime('%d%m')}_ep:{epoch}.pth\"\n",
        "  checkpoint = {'epoch': epoch, \n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss_list': loss_list, \n",
        "              'accuracy_list': accuracy_list,\n",
        "              'lr': lr,\n",
        "              'train_bs' : train_bs}\n",
        "  torch.save(checkpoint, PATH)\n",
        "\n",
        "## function to load checkpoint of linear classifier\n",
        "def load_checkpoint_classifier(name,date, e):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/classifier_{name}_{date}_ep:{e}.pth\"\n",
        "  checkpoint = torch.load(PATH)\n",
        "\n",
        "  epoch = checkpoint['epoch']\n",
        "  train_loss = checkpoint['loss_list'][-1]\n",
        "  max_accuracy = max(checkpoint['accuracy_list'])\n",
        "\n",
        "  print(\"checkpoint:\", date,\" Epoch: \", epoch, \" Max_accuracy: \",accuracy,\" Loss: \", train_loss)\n",
        "  return checkpoint\n",
        "\n",
        "\n",
        "\n",
        "## Function to do LINEAR EVALUATION of a model trained with SimSiam\n",
        "def Linear_evaluation(trained_model, train_set, test_set, SaveName=None, epochs = 10, train_bs = 512, lr=None, backbone = \"FCN\"):\n",
        "  #trained model: a model prevoously trained with SimSiam\n",
        "  #classification_dataset: The dataset to train and evaluate the classification \n",
        "  \n",
        "  #dataset = classification_dataset.Datasets[0]\n",
        "  #train_class, test_class = data_split(classification_dataset, train_size=0.5) #Split into training and test set for classifier\n",
        "\n",
        "  if(lr==None):\n",
        "    base_lr = 0.05\n",
        "    lr = (base_lr*train_bs)/256\n",
        "\n",
        "  train_loader = DataLoader(train_set, batch_size = train_bs, shuffle=True)\n",
        "  test_loader = DataLoader(test_set, batch_size = train_bs, shuffle=True)\n",
        "\n",
        "  classes = test_set.classes\n",
        " \n",
        "  ### Linear evaluation on the trained model ###\n",
        "  #Get the frozen backbone of the trained model\n",
        "  Backbone_model = trained_model.backbone\n",
        "  Backbone_model = Backbone_model.to(device)\n",
        "  Backbone_model = nn.DataParallel(Backbone_model)\n",
        "\n",
        "  #Linear classifier to train in top of the trained frozen model\n",
        "  Linear = nn.Linear(128, classes).to(device)\n",
        "  optimizer_linear = torch.optim.Adam(Linear.parameters(), lr=lr)\n",
        "  accuracies_linear = []\n",
        "  losses_linear = []\n",
        "\n",
        "  ### Linear evaluation on a randomly initialized mode√∏ ## (to compare)\n",
        "  if (backbone == \"ResNet\"):\n",
        "    Random_model = ResNet()\n",
        "  else:\n",
        "    Random_model = FCN()\n",
        "\n",
        "  Random_model = Random_model.to(device)\n",
        "  Linear_random = nn.Linear(128, classes).to(device) #Linear classifier to train on top of random model\n",
        "  optimizer_random = torch.optim.Adam(Linear_random.parameters(), lr=lr)\n",
        "  accuracies_random = []\n",
        "  losses_random = []\n",
        "\n",
        "\n",
        "  for e in range(epochs):\n",
        "    Backbone_model.eval()\n",
        "    Linear.train()\n",
        "    loss_list_linear = []\n",
        "\n",
        "    Random_model.eval()\n",
        "    Linear_random.train()\n",
        "    loss_list_random = []\n",
        "\n",
        "    for batch in enumerate(train_loader):\n",
        "      time_series_batch = batch[1]['time_series']\n",
        "      label_batch = batch[1]['label']\n",
        "\n",
        "      #Train linear classifier on top of backbone model\n",
        "      Linear.zero_grad()\n",
        "      with torch.no_grad():\n",
        "        feature = Backbone_model(time_series_batch.to(device))\n",
        "      pred = Linear(feature.to(device))\n",
        "      loss_linear = F.cross_entropy(pred, label_batch)\n",
        "      loss_linear.backward()\n",
        "      optimizer_linear.step()\n",
        "      loss_list_linear.append(loss_linear.item())\n",
        "\n",
        "      Linear_random.zero_grad()\n",
        "      with torch.no_grad():\n",
        "        feature_random = Random_model(time_series_batch.to(device))\n",
        "      pred_random = Linear_random(feature_random.to(device))\n",
        "      loss_random = F.cross_entropy(pred_random, label_batch)\n",
        "      loss_random.backward()\n",
        "      optimizer_random.step()\n",
        "      loss_list_random.append(loss_random.item())\n",
        "      \n",
        "    Loss_linear = np.mean(loss_list_linear)\n",
        "    losses_linear.append(Loss_linear)\n",
        "    accuracy_linear = evaluate_classifier(test_loader, Backbone_model, Linear)\n",
        "    accuracies_linear.append(accuracy_linear)\n",
        "\n",
        "    Loss_random = np.mean(loss_list_random)\n",
        "    losses_random.append(Loss_random)\n",
        "    accuracy_random = evaluate_classifier(test_loader, Random_model, Linear_random)\n",
        "    accuracies_random.append(accuracy_random)\n",
        "\n",
        "\n",
        "\n",
        "    #print(\"e:  \", e, \"SimSiam : accuracy; \",accuracy_linear, \" loss; \",Loss_linear )\n",
        "    #print(\"e:  \", e, \" dataset: \", dataset,\"  SimSiam:   accuracy; \",accuracy_linear, \" loss; \",Loss_linear, \"   Random : accuracy; \", accuracy_random, \" loss; \", Loss_random)\n",
        "\n",
        "  results = [dataset, \"linear eval\", e , train_bs, lr, accuracies_linear[-1], max(accuracies_linear), losses_linear[-1], max(accuracies_random)]\n",
        "  print(results)\n",
        "\n",
        "  if (SaveName !=None):\n",
        "    save_checkpoint_classifier(SaveName, Linear, e, optimizer_linear, losses_linear, accuracies_linear, lr, train_bs) #Save checkpoint\n",
        "  \n",
        "  return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQzBUYtUpSMf"
      },
      "source": [
        "##### Linear evaluation on each dataset of the big dataset #####\n",
        "\n",
        "#Big_dataset = [\"ChlorineConcentration\", \"Two_Patterns\", \"yoga\", \"uWaveGestureLibrary_X\", \"NonInvasiveFatalECG_Thorax1\", \"ECG5000\", \"FordA\", \"FordB\"]\n",
        "#dat = [\"ChlorineConcentration\"]\n",
        "\n",
        "#Load trained model\n",
        "Datasets = [\"ChlorineConcentration\", \"ECG5000\", \"ElectricDevices\", \"FordA\", \"FordB\", \"Two_Patterns\", \"wafer\", \"yoga\"]\n",
        "\n",
        "#SaveNames = [\"Window38\",\"Window38_sep\",\"Horizon03_sep\" ]\n",
        "#SaveNames = [\"Horizon03_sep\"]\n",
        "#SaveName = \"Window38\"\n",
        "#SaveName = \"Window38_sep\"\n",
        "#SaveName = \"Horizon03_sep\"\n",
        "#epoch = 99\n",
        "#date = 2704\n",
        "\n",
        "SaveNames = [\"ResNet_W38\"]\n",
        "epoch = 99\n",
        "date = 2904\n",
        "\n",
        "#backbone = \"FCN\"\n",
        "backbone = \"ResNet\"\n",
        "\n",
        "batch_size = 40\n",
        "#lr = 0.001\n",
        "lr=None\n",
        "labeled_sizes = [0.2,0.1,0.05]\n",
        "#labeled_sizes = [0.1]\n",
        "\n",
        "rounds = 10\n",
        "epochs = 40\n",
        "\n",
        "Results = []\n",
        "for SaveName in SaveNames:\n",
        "  checkpoint = load_checkpoint(SaveName, date, epoch)\n",
        "  trained_model, optimizer_model, epoch_model = load_trained_model(checkpoint, backbone=backbone)\n",
        "\n",
        "  for dataset in Datasets:\n",
        "    for labeled_size in labeled_sizes:\n",
        "      Train_set = Timeseries_Dataset([dataset], train=True, Save=None, transform = transforms.Compose( [ ToTensor()] ))\n",
        "      N_train = len(Train_set)\n",
        "\n",
        "      Train_set.shuffle()\n",
        "      Train_set_labeled = Train_set[:int(N_train*labeled_size)]\n",
        "\n",
        "      Test_set = Timeseries_Dataset([dataset], train=False, Save=None, transform = transforms.Compose( [ ToTensor()] ))\n",
        "\n",
        "      for round in range(rounds):\n",
        "        Train_set_labeled.shuffle()\n",
        "        Test_set.shuffle()\n",
        "        \n",
        "        Save_checkpoint_name = None\n",
        "\n",
        "        results = Linear_evaluation(trained_model, Train_set_labeled, Test_set, SaveName=Save_checkpoint_name, epochs = epochs , train_bs = batch_size, lr=lr, backbone = backbone)\n",
        "        results.append(SaveName)\n",
        "        results.append(date)\n",
        "        results.append(labeled_size)\n",
        "        \n",
        "        Results.append(results)\n",
        "  \n",
        "    #column_names = [\"Dataset\", \"Type\", \"epochs\", \"batch_size\", \"lr\", \"Accuracy\", \"Max Accuracy\", \"Loss\", \"Random model accuracy\", \"trained model used\", \"date model\", \"labeled size\"]\n",
        "    #to_Excel(Results, column_names, \"lin_eval_\"+SaveName+\".xlsx\")\n",
        "\n",
        "\n",
        "\n",
        "column_names = [\"Dataset\", \"Type\", \"epochs\", \"batch_size\", \"lr\", \"Accuracy\", \"Max Accuracy\", \"Loss\", \"Random model accuracy\", \"trained model used\", \"date model\", \"labeled size\"]\n",
        "name = \"_Linear_\"+SaveNames[0]\n",
        "to_Excel(Results, column_names, \"lin_eval_\"+name+\".xlsx\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}