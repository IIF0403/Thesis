{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "OnlyFineTuning(u19).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN3Rjedmu/z9aK+sQgj1K8A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IIF0403/Thesis/blob/main/OnlyFineTuning(u19).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Kbtvs3gINM"
      },
      "source": [
        "#Check GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd07YtQtkrm3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "from google.colab import drive\n",
        "from torch.nn.utils.rnn import pack_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "import random\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGHV1EGOY_Zi",
        "outputId": "9d7826aa-88b4-4d9f-d79e-d3e4195a71cd"
      },
      "source": [
        "#Importing Dataset class from other ipynb file\n",
        "!pip install import_ipynb\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "#!ls\n",
        "import import_ipynb\n",
        "from SimSiam_training import *\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaDBoUdqUbVY"
      },
      "source": [
        "### Some helping functions ###\n",
        "\n",
        "#Function to calculate Accuracy score\n",
        "def Accuracy_score(labels, preds):\n",
        "  if len(labels)!=len(preds):\n",
        "    print(\"sizes does not match\")\n",
        "  else:\n",
        "    total=0\n",
        "    correct=0\n",
        "    for i in range(len(labels)):\n",
        "      if labels[i]==preds[i]:\n",
        "        correct+=1\n",
        "      total+=1\n",
        "    return (correct/total)\n",
        "\n",
        "## Function to calculate classification_accuracy of a classifier given a frozen backbone model the test set\n",
        "def evaluate_classifier(test_loader, backbone_model, classifier):\n",
        "  classifier.eval()\n",
        "  accuracies = []\n",
        "  for batch in enumerate(test_loader):\n",
        "    time_series_batch = batch[1]['time_series']\n",
        "    label_batch = batch[1]['label']\n",
        "\n",
        "    with torch.no_grad():\n",
        "      feature = backbone_model(time_series_batch)\n",
        "      y_hat = classifier(feature)\n",
        "      pred = torch.max(y_hat,1)[1]\n",
        "      #print(\"pred: \", pred)\n",
        "      #print(\"true: \", label_batch)\n",
        "      accuracy = Accuracy_score(label_batch, pred) \n",
        "      accuracies.append(accuracy)\n",
        "  \n",
        "  Accuracy = np.mean(accuracies)\n",
        "  return Accuracy\n",
        "\n",
        "## Function to calculate classification_accuracy of a model (compplete model) given the test set\n",
        "def evaluate_model(test_loader, model):\n",
        "  model.eval()\n",
        "  accuracies = []\n",
        "  for batch in enumerate(test_loader):\n",
        "    time_series_batch = batch[1]['time_series']\n",
        "    label_batch = batch[1]['label']\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y_hat = model(time_series_batch)\n",
        "      pred = torch.max(y_hat,1)[1]\n",
        "      #print(\"pred: \", pred)\n",
        "      #print(\"true: \", label_batch)\n",
        "      accuracy = Accuracy_score(label_batch, pred) \n",
        "      accuracies.append(accuracy)\n",
        "  \n",
        "  Accuracy = np.mean(accuracies)\n",
        "  return Accuracy\n",
        "\n",
        "\n",
        "## function to save checkpoint of linear classifier\n",
        "def save_checkpoint_classifier(SaveName, model, epoch, optimizer, loss_list, accuracy_list, lr, train_bs):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/classifier_{SaveName}_{datetime.now().strftime('%d%m')}_ep:{epoch}.pth\"\n",
        "  checkpoint = {'epoch': epoch, \n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss_list': loss_list, \n",
        "              'accuracy_list': accuracy_list,\n",
        "              'lr': lr,\n",
        "              'train_bs' : train_bs}\n",
        "  torch.save(checkpoint, PATH)\n",
        "\n",
        "## function to load checkpoint of linear classifier\n",
        "def load_checkpoint_classifier(name,date, e):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/classifier_{name}_{date}_ep:{e}.pth\"\n",
        "  checkpoint = torch.load(PATH)\n",
        "\n",
        "  epoch = checkpoint['epoch']\n",
        "  train_loss = checkpoint['loss_list'][-1]\n",
        "  max_accuracy = max(checkpoint['accuracy_list'])\n",
        "\n",
        "  print(\"checkpoint:\", date,\" Epoch: \", epoch, \" Max_accuracy: \",accuracy,\" Loss: \", train_loss)\n",
        "  return checkpoint\n",
        "\n",
        "#Function to print value rounded to 4 desimals \n",
        "def rounded(value):\n",
        "  format = \"{:.4f}\".format(value)\n",
        "  float_value = float(format)\n",
        "  return float_value\n",
        "\n",
        "\n",
        "#Function to plot Accuracy or Loss over epochs\n",
        "def plot_progress(res, title):\n",
        "  N = len(res)\n",
        "  epochs = [i for i in range(N)]\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, res)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.title(title)\n",
        "\n",
        "  plt.savefig(title+\".png\")\n",
        "  files.download(title+\".png\") \n",
        "  plt.show()\n",
        "\n",
        "#### FineTune network model ####\n",
        "#Combines the Backbone model and the Linear to a complete model for finetuning\n",
        "class FineTuneModel(nn.Module):\n",
        "  def __init__(self, Backbone, Linear):\n",
        "    super(FineTuneModel, self).__init__()\n",
        "    self.Backbone = Backbone\n",
        "    self.Linear = Linear\n",
        "  \n",
        "  def forward(self, x):\n",
        "    feature = self.Backbone(x)\n",
        "    pred = self.Linear(feature)\n",
        "    return pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGIwlmgYtlwU"
      },
      "source": [
        "\n",
        "## Function to do FineTuning \n",
        "def FineTuning(trained_model, train_set, test_set, SaveName=None, epochs_Linear = 10, epochs_finetune=10, train_bs = 40, lr_linear=0.001, lr_finetune=0.001, backbone = \"FCN\"):\n",
        "  #trained model: a model previously trained with SimSiam\n",
        "  #train_set, test_set: The datasets to train and evaluate the classification \n",
        "  \n",
        "  dataset = train_set.Datasets[0]\n",
        "  classes = test_set.classes\n",
        "\n",
        "  Results = []\n",
        "\n",
        "  train_loader = DataLoader(train_set, batch_size = train_bs, shuffle=True)\n",
        "  test_loader = DataLoader(test_set, batch_size = train_bs, shuffle=True)\n",
        "\n",
        "  \n",
        "  #Get the frozen backbone of the trained model\n",
        "  Backbone_model = trained_model.backbone\n",
        "  Backbone_model = Backbone_model.to(device)\n",
        "  Backbone_model = nn.DataParallel(Backbone_model)\n",
        "\n",
        "  #Linear classifier to train in top of the trained frozen model\n",
        "  lr = lr_linear\n",
        "  Linear = nn.Linear(128, classes).to(device)\n",
        "  optimizer_linear = torch.optim.Adam(Linear.parameters(), lr=lr)\n",
        "  accuracies_linear = []\n",
        "  losses_linear = []\n",
        "\n",
        "  ########## Train Linear layer on frozen Backbone ############\n",
        "  print(\"Training Linear Layer\")\n",
        "  for e in range(epochs_Linear):\n",
        "    Backbone_model.eval()\n",
        "    Linear.train()\n",
        "    loss_list_linear = []\n",
        "\n",
        "    for batch in enumerate(train_loader):\n",
        "      time_series_batch = batch[1]['time_series']\n",
        "      label_batch = batch[1]['label']\n",
        "\n",
        "      #Train linear classifier on top of backbone model\n",
        "      Linear.zero_grad()\n",
        "      with torch.no_grad():\n",
        "        feature = Backbone_model(time_series_batch.to(device))\n",
        "      pred = Linear(feature.to(device))\n",
        "      loss_linear = F.cross_entropy(pred, label_batch)\n",
        "      loss_linear.backward()\n",
        "      optimizer_linear.step()\n",
        "      loss_list_linear.append(loss_linear.item())\n",
        "      \n",
        "    Loss_linear = np.mean(loss_list_linear)\n",
        "    losses_linear.append(Loss_linear)\n",
        "    accuracy_linear = evaluate_classifier(test_loader, Backbone_model, Linear)\n",
        "    accuracies_linear.append(accuracy_linear)\n",
        "\n",
        "    print(\"e:  \", e, \" dataset: \", dataset,\"  SimSiam:   accuracy; \", rounded(accuracy_linear), \" loss; \",rounded(Loss_linear))\n",
        "\n",
        "  results_linear = [dataset, \"LinearEval\", e , train_bs, lr, accuracies_linear[-1], max(accuracies_linear), losses_linear[-1]]\n",
        "  Results.append(results_linear)\n",
        "\n",
        "  #############################################################\n",
        "\n",
        "  ############### Unfreeze and train the whole network ##################\n",
        "  print(\"Fine tuning the whole network\")\n",
        "\n",
        "  #Get a copy of the trained frozen backbone and the trained Linear classifier\n",
        "  Backbone_finetune = nn.DataParallel(Backbone_model).to(device)\n",
        "  Linear_finetune = nn.DataParallel(Linear).to(device)\n",
        "\n",
        "  FineTune_model = FineTuneModel(Backbone_finetune, Linear_finetune) #Combine Backbone and Linear to a single model for finetuning\n",
        "\n",
        "  lr_FineTune =lr_finetune\n",
        "  optimizer_FineTune = torch.optim.Adam(FineTune_model.parameters(), lr=lr_FineTune)\n",
        "\n",
        "  losses_FineTune = []\n",
        "  accuracies_FineTune = []\n",
        "\n",
        "  for e in range(epochs_finetune):\n",
        "    FineTune_model.train()\n",
        "    loss_list_Finetune = []\n",
        "\n",
        "    for batch in enumerate(train_loader):\n",
        "      time_series_batch = batch[1]['time_series']\n",
        "      label_batch = batch[1]['label']\n",
        "\n",
        "      #Train the FineTune model\n",
        "      FineTune_model.zero_grad()\n",
        "      pred = FineTune_model(time_series_batch.to(device))\n",
        "\n",
        "      loss_FineTune = F.cross_entropy(pred, label_batch)\n",
        "      loss_FineTune.backward()\n",
        "      optimizer_FineTune.step()\n",
        "      loss_list_Finetune.append(loss_FineTune.item())\n",
        "\n",
        "\n",
        "    Loss_FineTune = np.mean(loss_list_Finetune)\n",
        "    losses_FineTune.append(Loss_FineTune)\n",
        "    accuracy_FineTune = evaluate_model(test_loader, FineTune_model)\n",
        "    accuracies_FineTune.append(accuracy_FineTune)\n",
        "\n",
        "    print(\"e:  \", e, \" dataset: \", dataset,\"  SimSiam:   accuracy; \",rounded(accuracy_FineTune), \" loss; \",rounded(Loss_FineTune))\n",
        "\n",
        "    if (e in [10,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400]):\n",
        "      results_finetune = [dataset, \"FineTuning\", e , train_bs, lr_FineTune, accuracies_FineTune[-1], max(accuracies_FineTune), losses_FineTune[-1]]\n",
        "      Results.append(results_finetune)\n",
        "\n",
        "  results_finetune = [dataset, \"FineTuning\", e , train_bs, lr_FineTune, accuracies_FineTune[-1], max(accuracies_FineTune), losses_FineTune[-1]]\n",
        "  Results.append(results_finetune)\n",
        "\n",
        "  #Plotting Accuracy and Loss over epochs\n",
        "  plot_progress(accuracies_FineTune, \"Accuracy_FineTune_\"+dataset)\n",
        "  plot_progress(losses_FineTune, \"Loss_FineTune_\"+dataset)\n",
        "\n",
        "  if (SaveName !=None):\n",
        "    save_checkpoint_classifier(SaveName, FineTuneModel, e, optimizer_FineTune, losses_FineTune, accuracies_FineTune, lr_FineTune, train_bs) #Save checkpoint\n",
        "\n",
        "  return Results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeJph1lYqZxJ"
      },
      "source": [
        "##### FineTuning on each dataset of the big dataset #####\n",
        "\n",
        "#Big_dataset = [\"ChlorineConcentration\", \"Two_Patterns\", \"yoga\", \"uWaveGestureLibrary_X\", \"NonInvasiveFatalECG_Thorax1\", \"ECG5000\", \"FordA\", \"FordB\"]\n",
        "#dat = [\"ChlorineConcentration\"]\n",
        "\n",
        "#Load trained model\n",
        "#Datasets = [\"ChlorineConcentration\", \"ECG5000\", \"ElectricDevices\", \"FordA\", \"FordB\", \"Two_Patterns\", \"wafer\", \"yoga\"]\n",
        "Datasets = [\"ChlorineConcentration\"]\n",
        "\n",
        "#SaveNames = [\"Window38\",\"Window38_sep\",\"Horizon03_sep\" ]\n",
        "#SaveNames = [\"Horizon03_sep\"]\n",
        "#SaveName = \"Window38_sep\"\n",
        "#SaveName = \"Horizon03_sep\"\n",
        "\n",
        "SaveNames = [\"Window38\"]\n",
        "epoch = 99\n",
        "date = 2704\n",
        "\n",
        "#SaveNames = [\"ResNet_W38\"]\n",
        "#epoch = 99\n",
        "#date = 2904\n",
        "\n",
        "backbone = \"FCN\"\n",
        "#backbone = \"ResNet\"\n",
        "\n",
        "batch_size = 40\n",
        "#lr = 0.001\n",
        "lr=None\n",
        "#labeled_sizes = [0.2,0.1,0.05]\n",
        "labeled_sizes = [0.1]\n",
        "\n",
        "rounds = 1\n",
        "epochs_Linear = 40\n",
        "epochs_FineTune = 200\n",
        "\n",
        "lr_linear= 0.01\n",
        "lr_finetune= 0.05\n",
        "\n",
        "\n",
        "Results = []\n",
        "\n",
        "for SaveName in SaveNames:\n",
        "  checkpoint = load_checkpoint(SaveName, date, epoch)\n",
        "  trained_model, optimizer_model, epoch_model, lr_model = load_trained_model(checkpoint, backbone=backbone)\n",
        "\n",
        "  for dataset in Datasets:\n",
        "    for labeled_size in labeled_sizes:\n",
        "      Train_set = Timeseries_Dataset([dataset], train=True, Save=None, transform = transforms.Compose( [ ToTensor()] ))\n",
        "      N_train = len(Train_set)\n",
        "\n",
        "      Train_set.shuffle()\n",
        "      Train_set_labeled = Train_set[:int(N_train*labeled_size)]\n",
        "\n",
        "      Test_set = Timeseries_Dataset([dataset], train=False, Save=None, transform = transforms.Compose( [ ToTensor()] ))\n",
        "\n",
        "      for round in range(rounds):\n",
        "        Train_set_labeled.shuffle()\n",
        "        Test_set.shuffle()\n",
        "        \n",
        "        Save_checkpoint_name = None\n",
        "\n",
        "        Results_1 = FineTuning(trained_model, Train_set_labeled, Test_set, SaveName=Save_checkpoint_name, epochs_Linear = epochs_Linear, epochs_finetune=epochs_FineTune, train_bs = batch_size, lr_linear=lr_linear, lr_finetune=lr_finetune, backbone = backbone)\n",
        "        \n",
        "        for res in Results_1:\n",
        "          result = res\n",
        "          result.append(SaveName)\n",
        "          result.append(date)\n",
        "          result.append(labeled_size)\n",
        "\n",
        "          Results.append(result)\n",
        "  \n",
        "    column_names = [\"Dataset\", \"Type\", \"epochs\", \"batch_size\", \"lr\", \"last_Acc\", \"Model Accuracy\", \"Loss Model\", \"trained model used\", \"date model\", \"labeled size\"]    \n",
        "    to_Excel(Results, column_names, \"FineTune_\"+SaveName+\"_\"+dataset+\".xlsx\")\n",
        "\n",
        "\n",
        "column_names = [\"Dataset\", \"Type\", \"epochs\", \"batch_size\", \"lr\", \"last_Acc\", \"Model Accuracy\", \"Loss Model\", \"trained model used\", \"date model\", \"labeled size\"]\n",
        "\n",
        "name = \"_FineTune_\"+SaveName+\"_all\"\n",
        "to_Excel(Results, column_names, name+\".xlsx\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJSpj_xVmGN0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}