{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimSiam_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnFfM/qGJFXGUxLkxgjmo/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IIF0403/Thesis/blob/main/SimSiam_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Kbtvs3gINM"
      },
      "source": [
        "#Check GPU\n",
        "#gpu_info = !nvidia-smi\n",
        "#gpu_info = '\\n'.join(gpu_info)\n",
        "#if gpu_info.find('failed') >= 0:\n",
        "  #print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  #print('and then re-execute this cell.')\n",
        "#else:\n",
        "  #print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd07YtQtkrm3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "from google.colab import drive\n",
        "from torch.nn.utils.rnn import pack_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGCgMeDHklIM"
      },
      "source": [
        "##### DATA #######\n",
        "\n",
        "#Function to load a dataset from my github or Google Drive\n",
        "def load_dataset(Dataset, Drive, train):\n",
        "  #Dataset: The name of the dataset to load. Example: \"ECG5000\"\n",
        "\n",
        "  #If Drive = False; Load dataset from github and convert the data to the correct format\n",
        "  if (Drive==False): \n",
        "    print(\"loading \", Dataset, \"from github\")\n",
        "    #github URL\n",
        "    url_raw = 'https://raw.githubusercontent.com/IIF0403/Thesis/main/data/'\n",
        "    url_train = url_raw + Dataset+'/'+Dataset+'_TRAIN'\n",
        "    url_test = url_raw + Dataset+'/'+Dataset+'_TEST'\n",
        "\n",
        "    #Loading the data\n",
        "    data_train = pd.read_csv(url_train,header=None)\n",
        "    data_test = pd.read_csv(url_test, header=None)\n",
        "    data = pd.concat((data_train, data_test))\n",
        "\n",
        "    #Want all datasets to have classes as integers starting from 0 \n",
        "    Y = data.values[:,0]\n",
        "    classes = len(np.unique(Y))\n",
        "    Y_transformed = ( (Y-Y.min())/(Y.max()-Y.min()) )*(classes-1)\n",
        "    data[data.columns[0]] = Y_transformed\n",
        "\n",
        "    #Inserting the name of the dataset as a column (for later use, when several datasets will be combined)\n",
        "    data.insert(loc=0, column = \"Dataset\", value=Dataset) \n",
        "\n",
        "    #Inserting the length of the time series T as a column (for later use)\n",
        "    T = data.shape[1]-2 #The length of the time series\n",
        "    data.insert(loc=1, column = \"T\", value = T)\n",
        "    return data, classes\n",
        "\n",
        "  ## If Drive = True; Load data from Google Drive, the data is already converted to the correct format\n",
        "  else: \n",
        "    print(\"Loading '\",Dataset,\"' from Google Drive\")\n",
        "    drive.mount('/content/drive')\n",
        "    #data = pd.read_csv('/content/drive/MyDrive/Datasets/'+Dataset+'.csv')\n",
        "\n",
        "    if (train == True): #Load the train_data\n",
        "      data = pd.read_csv('/content/drive/MyDrive/Datasets/'+Dataset+'_train.csv')\n",
        "    else: #Load the test data\n",
        "      data = pd.read_csv('/content/drive/MyDrive/Datasets/'+Dataset+'_test.csv')\n",
        "\n",
        "    classes = len(np.unique(data.values[:,2]))\n",
        "\n",
        "    return data, classes\n",
        "\n",
        "\n",
        "### Function to make a big dataset out of several datasets loaded from github ###\n",
        "def make_big_dataset(Datasets, Drive, train):\n",
        "  #Datasets: A list of the name of the datasets to load. Example: [\"ECG5000\", \"FordA\", \"FordB\"]\n",
        "\n",
        "  #Loading first dataset\n",
        "  data, classes =  load_dataset(Datasets[0], Drive, train)\n",
        "  used_classes = classes #keeping track of the class-labels already used\n",
        "\n",
        "  #Loading each of the datasets in the list and combining them all togehter in a big dataset\n",
        "  for i in range(1, len(Datasets)):\n",
        "    #loading the i´th dataset\n",
        "    Dataset = Datasets[i]\n",
        "    dataset, classes= load_dataset(Dataset, Drive, train)\n",
        "\n",
        "    #Need to change the class-labels such that all the class-labels of the different datasets differ from eachother\n",
        "    labels = dataset.values[:,2]\n",
        "    transformed_labels = labels + used_classes \n",
        "    dataset[dataset.columns[2]] = transformed_labels\n",
        "    \n",
        "    used_classes += classes #keeping track of the class-labels already used\n",
        "\n",
        "    data = pd.concat((data, dataset)) #adding the new dataset to the big dataset\n",
        "  \n",
        "  return data, used_classes\n",
        "\n",
        "#Function to save a new train and test dataset in drive\n",
        "def save_train_test(Dataset):\n",
        "  Dataset = Datasets[7]\n",
        "  data, classes = load_dataset(Dataset)\n",
        "  data_train, data_test = train_test_split(data, test_size=0.2)\n",
        "\n",
        "  #print(Dataset,\": \",len(data),\" train: \",len(data_train),\" test: \", len(data_test) )\n",
        "\n",
        "  Save = Dataset\n",
        "  drive.mount('/content/drive')\n",
        "  data_train.to_csv('/content/drive/MyDrive/Datasets/'+Save+'_train.csv', index=False)\n",
        "  data_test.to_csv('/content/drive/MyDrive/Datasets/'+Save+'_test.csv', index=False)\n",
        "\n",
        "\n",
        "### Dataset class ###\n",
        "class Timeseries_Dataset(Dataset):\n",
        "  def __init__(self, Datasets, train = True, Drive=True, Save=None, transform=None):\n",
        "    #Datasets: a list with the name of the datasets, can be a list og several or one dataset. If it is several datasets, they will be made into one big dataset\n",
        "    #Drive: True means loading the already saved dataset from google drive, false means loading data from github\n",
        "    #Save: name of new dataset, if we want to save the new dataset to google drive\n",
        "    #transform: a given transformation of the data\n",
        "\n",
        "    ##Loading the data\n",
        "    #If Datasets contains several datasets, load and combine the datasets\n",
        "    if (len(Datasets)>1):\n",
        "      data, classes = make_big_dataset(Datasets, Drive, train)\n",
        "    \n",
        "    #If Datasets only contains one dataset, load the dataset\n",
        "    else:\n",
        "      Dataset = Datasets[0]\n",
        "      data, classes = load_dataset(Dataset, Drive, train)\n",
        "\n",
        "    #Save new dataset to Google drive as 'name' if Save = 'name' and not None\n",
        "    if (Save!=None): #Save the new dataset to google drive as Save\n",
        "      print(\"Saving new dataset to google drive\")\n",
        "      drive.mount('/content/drive')\n",
        "      data.to_csv('/content/drive/MyDrive/Datasets/'+Save+'.csv', index=False)\n",
        "    \n",
        "\n",
        "    self.dataframe = data\n",
        "    self.transform = transform\n",
        "    self.classes = classes\n",
        "    self.Datasets = Datasets\n",
        "    \n",
        "  #defining the len(Dataset) function\n",
        "  def __len__(self): \n",
        "    return len(self.dataframe)\n",
        "\n",
        "  #defining the _getitem_ function which creates samples, such that when Dataset[i] is called; the i´th sample is returned\n",
        "  def __getitem__(self, key): \n",
        "\n",
        "    if isinstance(key, slice): # if given a slicing\n",
        "      start, stop, step = key.indices(len(self))\n",
        "      sliced = deepcopy(self)\n",
        "      sliced_data = self.dataframe.iloc[start:stop:step]\n",
        "      sliced.dataframe = sliced_data\n",
        "      return  sliced\n",
        "\n",
        "    else: #If given a single index \n",
        "      if torch.is_tensor(key):\n",
        "        key = key.tolist()\n",
        "      \n",
        "      #For one sample Dataset[i]:\n",
        "      dataframe = self.dataframe\n",
        "\n",
        "      label = dataframe.iloc[key, 2] #retrieveing the label\n",
        "      dataset = dataframe.iloc[key,0] #retrieveing the dataset-name\n",
        "      T = dataframe.iloc[key,1] #retrieveing the timeseries-length\n",
        "      time_series_with_nan = dataframe.iloc[key,3:].to_numpy() #retrieveing the timeseries (containing nan-values at the end)\n",
        "      time_series = time_series_with_nan[:T] #Removing nan_values at the end\n",
        "\n",
        "      sample = {'time_series': time_series, 'label': label, 'dataset': dataset, \"T\": T} #a sample is one timeseries with it's corresponding label (and som xtra information)\n",
        "\n",
        "      if self.transform: #transform sample\n",
        "        sample = self.transform(sample)\n",
        "\n",
        "      return sample\n",
        "  \n",
        "  def info(self):#Function to print information about the dataset\n",
        "    print(\"Datasets included: \", self.Datasets)\n",
        "    print(\"Number of classes : \", self.classes)\n",
        "    print(\"Size of dataset: \", len(self))\n",
        "  \n",
        "  def shuffle(self):\n",
        "    self.dataframe = self.dataframe.sample(frac = 1)\n",
        "  \n",
        "\n",
        "### Transformation class; segment timeseries into two augmentations\n",
        "class TwoSegments(object):\n",
        "  def __init__(self, horizon=0.3, window_gap=1, random_startpos = False, random_horizon=False, random_window_gap=False):\n",
        "    #horizon: horizon*T = window_length; the length of the two augmentations\n",
        "    #window_gap: the gap bewteen the two augmentations\n",
        "    #random_startpos: True means that the first augmentation starts at a random position\n",
        "    #random_horizon: True means that a random horizon is chosen\n",
        "    #random_window_gap: True means that a random window_gap is chosen\n",
        "\n",
        "    self.horizon = horizon\n",
        "    self.window_gap = window_gap\n",
        "    self.random_startpos = random_startpos\n",
        "    self.random_horizon = random_horizon\n",
        "    self.random_window_gap = random_window_gap\n",
        "        \n",
        "  def __call__(self, sample):\n",
        "\n",
        "    dataset = sample['dataset']\n",
        "    time_series = sample['time_series']\n",
        "    T = sample['T']\n",
        "    label = sample['label']\n",
        "\n",
        "    #horizon\n",
        "    if (self.random_horizon==True):\n",
        "      possible_horizons = [0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
        "      horizon = random.choice(possible_horizons) #draw a random horizon\n",
        "    else:\n",
        "      horizon = self.horizon\n",
        "\n",
        "    #window gap\n",
        "    if (self.random_window_gap==True):\n",
        "      possible_window_gaps =[0,1,2,3,4,5,6,7,8,9,10]\n",
        "      window_gap = random.choice(possible_window_gaps) #draw random window_gap\n",
        "    else:\n",
        "      window_gap = self.window_gap\n",
        "\n",
        "    #window_length = int(horizon*T) #length of each augmentation\n",
        "    window_length = 38\n",
        "\n",
        "    #finding start-position of the first augmentation\n",
        "    if (self.random_startpos == True): #if random start_position\n",
        "      max_possible_startposition = T-(2*window_length+window_gap) #the maximal start-position of the first augmentation\n",
        "      possible_startpossisions = [i for i in range(max_possible_startposition_aug1)] #The possible start positions of the first augmentation\n",
        "      start_pos = random.choice(possible_startpossisions) #draw a random startposition\n",
        "    else:\n",
        "      start_pos = 0 \n",
        "\n",
        "    #make the two augmentations of the timeseries\n",
        "    augmentation_1 = time_series[start_pos : (start_pos+window_length)]\n",
        "    augmentation_2 = time_series[(start_pos+window_length+window_gap) : (start_pos+window_length+window_gap)+window_length]\n",
        "\n",
        "    #create a new sample with the two augmentations\n",
        "    #new_sample = {'time_series': time_series, 'aug1': augmentation_1, 'aug2': augmentation_2, 'label': label, 'dataset': dataset, \"T\": T}\n",
        "    new_sample = {'aug1': augmentation_1, 'aug2': augmentation_2, 'label': label, 'dataset': dataset, \"T\": T}\n",
        "\n",
        "    return new_sample\n",
        "\n",
        "### Transformation class; convert into Tensor-data for PyTorch-use\n",
        "class ToTensor(object):\n",
        "  def __call__(self, sample):\n",
        "    dataset = sample['dataset']\n",
        "    T = sample['T']\n",
        "    label = sample['label']\n",
        "    #window_length = sample['window_length']\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if (len(sample)== 5):\n",
        "      #time_series = sample['time_series'].astype(float)\n",
        "      #time_series = time_series[np.newaxis,:] #tensor\n",
        "\n",
        "      aug1 = sample['aug1'].astype(float)\n",
        "      aug1 = aug1[np.newaxis,:] #tensor\n",
        "\n",
        "      aug2 = sample['aug2'].astype(float)\n",
        "      aug2 = aug2[np.newaxis,:] # tensor\n",
        "      \n",
        "      #label = label.astype(float)\n",
        "\n",
        "      #time_series_tensor = torch.tensor(time_series, dtype=torch.float32, device=device)\n",
        "      aug1_tensor = torch.tensor(aug1, dtype=torch.float32, device=device)\n",
        "      aug2_tensor = torch.tensor(aug2, dtype=torch.float32, device=device)\n",
        "      label_tensor = torch.tensor(label, dtype=torch.long, device=device)\n",
        "\n",
        "      #torch_sample = {'time_series': time_series_tensor, 'aug1': aug1_tensor, 'aug2': aug2_tensor, 'label': label_tensor, 'dataset': dataset, \"T\": T}\n",
        "      torch_sample = {'aug1': aug1_tensor, 'aug2': aug2_tensor, 'label': label_tensor, 'dataset': dataset, \"T\": T}\n",
        "\n",
        "\n",
        "    else:\n",
        "      time_series = sample['time_series'].astype(float)\n",
        "      time_series = time_series[np.newaxis,:] #tensor\n",
        "\n",
        "      label = label.astype(float)\n",
        "\n",
        "      time_series_tensor = torch.tensor(time_series, dtype=torch.float32, device=device)\n",
        "      label_tensor = torch.tensor(label, dtype=torch.long, device=device)\n",
        "\n",
        "      torch_sample = {'time_series': time_series_tensor, 'label': label_tensor, 'dataset': dataset, \"T\": T}\n",
        "\n",
        "    return torch_sample\n",
        "\n",
        "#Function to split into train and test dataset given a Timeseries_Dataset object\n",
        "def data_split(dat, train_size=0.7):\n",
        "  N = len(dat)\n",
        "  n = int(train_size*N)\n",
        "\n",
        "  dat.shuffle()\n",
        "  train = dat[:n]\n",
        "  test = dat[n:]\n",
        "\n",
        "  return train, test\n",
        "\n",
        "#function to save results in Excel\n",
        "def to_Excel(Results, columns, output_name):\n",
        "  dataframe = pd.DataFrame(Results, columns = columns)\n",
        "  # create excel writer object\n",
        "  writer = pd.ExcelWriter(output_name)\n",
        "  # write dataframe to excel\n",
        "  dataframe.to_excel(writer)\n",
        "  # save the excel\n",
        "  writer.save()\n",
        "  files.download(output_name)\n",
        "  print('DataFrame is written successfully to Excel File.') \n",
        "\n",
        "###################\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqSEyvR3oEhj"
      },
      "source": [
        "##### BACKBONE MODELS #######\n",
        "\n",
        "### FCN modellen fra prosjektoppgave\n",
        "class FCN(nn.Module):\n",
        "  def __init__(self, class_train=False, classes = 0):\n",
        "    super(FCN, self).__init__()\n",
        "    self.class_train= class_train\n",
        "    self.classes = classes\n",
        "\n",
        "    self.conv1 = nn.Conv1d(1, 128, 9, padding=(9 // 2))\n",
        "    #self.bnorm1 = nn.BatchNorm1d(128) \n",
        "    self.bnorm1 = nn.GroupNorm(1,128)\n",
        "\n",
        "    self.conv2 = nn.Conv1d(128, 256, 5, padding=(5 // 2))\n",
        "    #self.bnorm2 = nn.BatchNorm1d(256)\n",
        "    self.bnorm2 = nn.GroupNorm(1,256)\n",
        "\n",
        "    self.conv3 = nn.Conv1d(256, 128, 3, padding=(3 // 2))\n",
        "    #self.bnorm3 = nn.BatchNorm1d(128)\n",
        "    self.bnorm3 = nn.GroupNorm(1,128)\n",
        "\n",
        "    self.output_dim = 128\n",
        "\n",
        "    if (class_train==True):\n",
        "      self.classification_head = nn.Linear(128, classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    b1_class = F.relu(self.bnorm1(self.conv1(x)))\n",
        "    b2_class = F.relu(self.bnorm2(self.conv2(b1_class)))\n",
        "    b3_class = F.relu(self.bnorm3(self.conv3(b2_class)))\n",
        "\n",
        "    features_class = torch.mean(b3_class, 2) \n",
        "\n",
        "    if (self.class_train==True):\n",
        "      out_class = self.classification_head(features_class)\n",
        "      return out_class\n",
        "    else:\n",
        "      return features_class\n",
        "\n",
        "\n",
        "##Residual block to ResNet model\n",
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, in_maps, out_maps):\n",
        "    super(ResBlock, self).__init__()\n",
        "\n",
        "    self.in_maps = in_maps\n",
        "    self.out_maps = out_maps\n",
        "\n",
        "    self.conv1 = nn.Conv1d(self.in_maps,  self.out_maps, 9, padding=(9 // 2))\n",
        "    self.n1 = nn.GroupNorm(1,self.out_maps) #LayerNorm\n",
        "\n",
        "    self.conv2 = nn.Conv1d(self.out_maps, self.out_maps, 5, padding=(5 // 2))\n",
        "    self.n2 = nn.GroupNorm(1,self.out_maps) #LayerNorm\n",
        "\n",
        "    self.conv3 = nn.Conv1d(self.out_maps, self.out_maps, 3, padding=(3 // 2))\n",
        "    self.n3 = nn.GroupNorm(1,self.out_maps) #LayerNorm\n",
        "\n",
        "    self.output_dim = 128\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x   = F.relu(self.n1(self.conv1(x)))\n",
        "    inx = x\n",
        "    x   = F.relu(self.n2(self.conv2(x)))\n",
        "    x   = F.relu(self.n3(self.conv3(x))+inx)\n",
        "\n",
        "    return x\n",
        "\n",
        "#simple ResNet model\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResNet, self).__init__()\n",
        "    blocks  = [1,64,128,128]\n",
        "    self.blocks = nn.ModuleList()\n",
        "    for b,_ in enumerate(blocks[:-1]):\n",
        "        self.blocks.append(ResBlock(*blocks[b:b+2]))\n",
        "\n",
        "    self.output_dim = 128\n",
        "         \n",
        "  def forward(self, x: torch.Tensor):\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "\n",
        "    x = torch.mean(x,dim=2)\n",
        "\n",
        "    #x = self.fc1(x)\n",
        "    #x = F.log_softmax(x,1)\n",
        "        \n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIEQRdOkR8od"
      },
      "source": [
        "### SIMPLE SIAMESE REPRESENTATION LEANRING MODELLEN ###\n",
        "\n",
        "## DISTANCE FUNCTION ##\n",
        "#The distance to minimize  (Negative Cosine similarity)\n",
        "def D(p, z):\n",
        "  z = z.detach() #stop gradient\n",
        "  p = p\n",
        "\n",
        "  neg_cosine_sim = - F.cosine_similarity(p, z, dim=1) #Negative cosine similarities (size: [bs])\n",
        "\n",
        "  return neg_cosine_sim.mean()  #return the mean of the negative cosine similarities\n",
        "\n",
        "\n",
        "## PROJECTION MLP ## ( in f(x))\n",
        "#has 3 FC layers with BN, the output FC has no ReLU, hidden FC is 2048-d\n",
        "class projection_MLP(nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim=2048, out_dim=2048): \n",
        "    super().__init__()\n",
        "    #Layer 1\n",
        "    self.FC1 = nn.Sequential(\n",
        "      nn.Linear(in_dim, hidden_dim),\n",
        "      nn.BatchNorm1d(hidden_dim),\n",
        "      nn.ReLU(inplace=True)  )\n",
        "    #Layer 2\n",
        "    self.FC2 = nn.Sequential(\n",
        "      nn.Linear(hidden_dim, hidden_dim),\n",
        "      nn.BatchNorm1d(hidden_dim),\n",
        "      nn.ReLU(inplace=True)  )\n",
        "    #Layer 3 (output)\n",
        "    self.FC3 = nn.Sequential(\n",
        "      nn.Linear(hidden_dim, out_dim),\n",
        "      nn.BatchNorm1d(hidden_dim) )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x1 = self.FC1(x)\n",
        "    x2 = self.FC2(x1)\n",
        "    x_out = self.FC3(x2)\n",
        "    return x_out\n",
        "\n",
        "\n",
        "## PREDICTION MLP ## h(z)\n",
        "#has 2 FC layers, only BN and ReLU on hidden layer (explained why in SimSiam paper). \n",
        "# input: z = f(x) dim = 2048\n",
        "# output: p = h(z) dim = 2048\n",
        "class prediction_MLP(nn.Module):\n",
        "  def __init__(self, in_dim=2048, hidden_dim=512, out_dim=2048): # bottleneck structure\n",
        "    super().__init__()\n",
        "    #Layer 1\n",
        "    self.FC1 = nn.Sequential(\n",
        "      nn.Linear(in_dim, hidden_dim),\n",
        "      nn.BatchNorm1d(hidden_dim),\n",
        "      nn.ReLU(inplace=True) )\n",
        "    #Layer 2 (output)\n",
        "    self.FC2 = nn.Linear(hidden_dim, out_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x1 = self.FC1(x)\n",
        "    x_out = self.FC2(x1)\n",
        "    return x_out\n",
        "\n",
        "\n",
        "## SIMSIAM MODEL ##\n",
        "class SimSiam(nn.Module):\n",
        "  #def __init__(self, backbone=FCN()): #take backbone as input\n",
        "  def __init__(self, backbone=\"FCN\", window_length=None): #take backbone as input\n",
        "    super().__init__()\n",
        "    self.name = SimSiam\n",
        "\n",
        "    if (backbone == \"ResNet\"):\n",
        "      backbone = ResNet()\n",
        "    else: \n",
        "      backbone = FCN()\n",
        "\n",
        "    self.backbone = backbone\n",
        "    self.projector = projection_MLP(backbone.output_dim) \n",
        "    \n",
        "    #Encoder; z = f(x)\n",
        "    self.encoder = nn.Sequential( \n",
        "      self.backbone,\n",
        "      self.projector )\n",
        "\n",
        "    #Predictor; p = h(z)\n",
        "    self.predictor = prediction_MLP()\n",
        "  \n",
        "  def forward(self, x1, x2):\n",
        "    #x1, x2: augmentations of x\n",
        "    f = self.encoder\n",
        "    h = self.predictor\n",
        "\n",
        "    z1, z2 = f(x1), f(x2)\n",
        "    p1, p2 = h(z1), h(z2)\n",
        "\n",
        "\n",
        "    #Symmetric loss\n",
        "    Loss = D(p1, z2)/2 + D(p2, z1)/2\n",
        "\n",
        "    return Loss\n",
        "\n",
        "###################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X_e8bGxvc8o"
      },
      "source": [
        "##### TRAINING A SIMSIAM MODEL #####\n",
        "\n",
        "### Function to train model\n",
        "def train(model, optimizer, train_data, train_epochs, old_epoch=0, train_bs=512, lr=0.001, SaveName=None):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  train_loader = DataLoader(train_data, batch_size = train_bs, shuffle=True)\n",
        "  train_losses =[]\n",
        "\n",
        "  for e in range(old_epoch+1, old_epoch+train_epochs):\n",
        "    losses =[]\n",
        "    model.train()\n",
        "    for batch in enumerate(train_loader):\n",
        "      x1_batch = batch[1]['aug1']\n",
        "      x2_batch = batch[1]['aug2']\n",
        "\n",
        "      model.zero_grad()\n",
        "      loss = model.forward(x1_batch, x2_batch).mean()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      losses.append(loss.item())\n",
        "  \n",
        "    train_loss = np.mean(losses)\n",
        "    print(\"e: \",e,\" Avg Loss: \", train_loss)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    if (SaveName!=None)and(e in [10,20,30,60,80]):\n",
        "      save_checkpoint(SaveName, model, e, optimizer, train_loss, lr, train_bs)\n",
        "\n",
        "  if SaveName!=None:\n",
        "    save_checkpoint(SaveName, model, e, optimizer, train_loss, lr, train_bs)\n",
        "\n",
        "### Function to save a checkpoint in Google Drive\n",
        "def save_checkpoint(name, model, epoch, optimizer, train_loss, lr, train_bs):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/{name}_{datetime.now().strftime('%d%m')}_ep:{epoch}.pth\"\n",
        "  checkpoint = {'epoch': epoch, \n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'train_loss': train_loss, \n",
        "              'lr': lr,\n",
        "              'train_bs' : train_bs}\n",
        "  torch.save(checkpoint, PATH) \n",
        "  \n",
        "### Function to load a previously saved checkpoint from Google Drive\n",
        "def load_checkpoint(name,date, e):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/{name}_{date}_ep:{e}.pth\"\n",
        "  checkpoint = torch.load(PATH)\n",
        "\n",
        "  epoch = checkpoint['epoch']\n",
        "  train_loss = checkpoint['train_loss']\n",
        "  #lr = checkpoint['lr']\n",
        "  #batch_size = checkpoint['batch_size']\n",
        "\n",
        "  #print(\"checkpoint:\", date,\" Epoch: \", epoch, \" Loss: \", train_loss, \"lr: \", lr, \"train_bs: \", train_bs)\n",
        "  print(\"checkpoint:\", date,\" Epoch: \", epoch, \" Loss: \", train_loss)\n",
        "  return checkpoint\n",
        "\n",
        "## Function to get a previously trained model from a checkpoint\n",
        "def load_trained_model(checkpoint, backbone= \"FCN\"):\n",
        "  epoch = checkpoint['epoch']\n",
        "  train_loss = checkpoint['train_loss']\n",
        "  lr = checkpoint['lr']\n",
        "  train_bs = checkpoint['train_bs']\n",
        "\n",
        "  #train_bs = 512\n",
        "  #base_lr = 0.05\n",
        "  #lr = (base_lr*train_bs)/256\n",
        "\n",
        "  model = SimSiam(backbone=backbone).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  \n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  model.eval()\n",
        "\n",
        "  return model.to(device), optimizer, epoch\n",
        "\n",
        "\n",
        "###################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLD7R9lzvKDZ"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  #Loading dataset\n",
        "  #Datasets = [\"ChlorineConcentration\", \"ECG5000\", \"ElectricDevices\", \"FordA\", \"FordB\", \"Two_Patterns\", \"wafer\", \"yoga\"]\n",
        "  #Train_dataset = Timeseries_Dataset(Datasets, train= True, Drive=True, transform = transforms.Compose( [TwoSegments(), ToTensor()] ))\n",
        "  #Train_dataset.info()\n",
        "\n",
        "  #window_length = Train_dataset[0]['window_length']\n",
        "\n",
        "  ### Training a SimSiam model\n",
        "  #SaveName = \"ResNetW38\"\n",
        "\n",
        "  #SaveName = \"ResNet_W38\"\n",
        "  #epochs = 100\n",
        "  #train_bs = 512\n",
        "\n",
        "  #base_lr = 0.05\n",
        "  #lr = (base_lr*train_bs)/256\n",
        "\n",
        "  #model = SimSiam(backbone=\"FCN\").to(device)\n",
        "  #model = SimSiam(backbone=\"ResNet\").to(device)\n",
        "\n",
        "  #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  #train(model, optimizer, Train_dataset, epochs, old_epoch=0, train_bs=train_bs, lr=lr, SaveName=SaveName)\n",
        "\n",
        "  #Load trained model\n",
        "  #Datasets = [\"ChlorineConcentration\", \"ECG5000\", \"ElectricDevices\", \"FordA\", \"FordB\", \"Two_Patterns\", \"wafer\", \"yoga\"]\n",
        "  #SaveName = \"Window38\"\n",
        "  #epoch = 99\n",
        "  #date = 2704\n",
        "\n",
        "  #checkpoint = load_checkpoint(SaveName, date, epoch)\n",
        "  #model, optimizer, epoch = load_trained_model(checkpoint)\n",
        "\n",
        "  #SaveName = \"ResNet_W38\"\n",
        "  #epoch = 99\n",
        "  #date = 2904\n",
        "\n",
        "  #checkpoint = load_checkpoint(SaveName, date, epoch)\n",
        "  #model, optimizer, epoch = load_trained_model(checkpoint, backbone =\"ResNet\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}