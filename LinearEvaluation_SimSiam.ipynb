{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LinearEvaluation_SimSiam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNp6MwNkhjwNDs95mtq3aei",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IIF0403/Thesis/blob/main/LinearEvaluation_SimSiam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd07YtQtkrm3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "from google.colab import drive\n",
        "from torch.nn.utils.rnn import pack_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "import random\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGHV1EGOY_Zi"
      },
      "source": [
        "#Importing Dataset class from other ipynb file \"SimSiam_training\"\n",
        "!pip install import_ipynb\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks'\n",
        "\n",
        "#!ls \n",
        "import import_ipynb\n",
        "from SimSiam_training import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaDBoUdqUbVY"
      },
      "source": [
        "##### LINEAR EVALUATION: training classifier on top of frozen model and evaluating #####\n",
        "\n",
        "#Function to calculate the accuracy score \n",
        "def Accuracy_score(labels, preds): \n",
        "  if len(labels)!=len(preds):\n",
        "    print(\"sizes does not match\")\n",
        "  else:\n",
        "    total=0\n",
        "    correct=0\n",
        "    for i in range(len(labels)):\n",
        "      if labels[i]==preds[i]:\n",
        "        correct+=1\n",
        "      total+=1\n",
        "    return (correct/total)\n",
        "\n",
        "\n",
        "#Function to print value rounded to 4 desimals \n",
        "def rounded(value):\n",
        "  format = \"{:.4f}\".format(value)\n",
        "  float_value = float(format)\n",
        "  return float_value\n",
        "\n",
        "## Function to calculate classification_accuracy given the test_set\n",
        "def evaluate_classifier(test_loader, backbone_model, classifier):\n",
        "  classifier.eval()\n",
        "  accuracies = []\n",
        "  for batch in enumerate(test_loader):\n",
        "    time_series_batch = batch[1]['time_series']\n",
        "    label_batch = batch[1]['label']\n",
        "\n",
        "    with torch.no_grad():\n",
        "      feature = backbone_model(time_series_batch)\n",
        "      y_hat = classifier(feature)\n",
        "      pred = torch.max(y_hat,1)[1]\n",
        "      #print(\"pred: \", pred)\n",
        "      #print(\"true: \", label_batch)\n",
        "      accuracy = Accuracy_score(label_batch, pred) \n",
        "      accuracies.append(accuracy)\n",
        "  \n",
        "  Accuracy = np.mean(accuracies)\n",
        "  return Accuracy\n",
        "\n",
        "\n",
        "## function to save checkpoint of linear classifier\n",
        "def save_checkpoint_classifier(SaveName, model, epoch, optimizer, loss_list, accuracy_list, lr, train_bs):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/classifier_{SaveName}.pth\"\n",
        "  checkpoint = {'epoch': epoch, \n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss_list': loss_list, \n",
        "              'accuracy_list': accuracy_list,\n",
        "              'lr': lr,\n",
        "              'train_bs' : train_bs}\n",
        "  torch.save(checkpoint, PATH)\n",
        "\n",
        "## function to load checkpoint of linear classifier\n",
        "def load_checkpoint_classifier(name):\n",
        "  drive.mount('/content/drive')\n",
        "  PATH = f\"/content/drive/MyDrive/checkpoints/classifier_{SaveName}.pth\"\n",
        "  checkpoint = torch.load(PATH)\n",
        "\n",
        "  epoch = checkpoint['epoch']\n",
        "  train_loss = checkpoint['loss_list'][-1]\n",
        "  max_accuracy = max(checkpoint['accuracy_list'])\n",
        "\n",
        "  print(\"checkpoint:\", date,\" Epoch: \", epoch, \" Max_accuracy: \",accuracy,\" Loss: \", train_loss)\n",
        "  return checkpoint\n",
        "  \n",
        "## Function to do LINEAR EVALUATION of a model trained with SimSiam\n",
        "def Linear_evaluation(trained_model, train_set, test_set, SaveName=None, epochs = 10, train_bs = 512, lr=None, backbone = \"FCN\"):\n",
        "  #trained model: a model prevoously trained with SimSiam\n",
        "  #classification_dataset: The dataset to train and evaluate the classification \n",
        "  \n",
        "  #dataset = classification_dataset.Datasets[0]\n",
        "  #train_class, test_class = data_split(classification_dataset, train_size=0.5) #Split into training and test set for classifier\n",
        "\n",
        "  if(lr==None):\n",
        "    base_lr = 0.05\n",
        "    lr = (base_lr*train_bs)/256\n",
        "\n",
        "  train_loader = DataLoader(train_set, batch_size = train_bs, shuffle=True)\n",
        "  test_loader = DataLoader(test_set, batch_size = train_bs, shuffle=True)\n",
        "\n",
        "  classes = test_set.classes\n",
        " \n",
        "  ### Linear evaluation on the trained model ###\n",
        "  #Get the frozen backbone of the trained model\n",
        "  Backbone_model = trained_model.backbone\n",
        "  Backbone_model = Backbone_model.to(device)\n",
        "  Backbone_model = nn.DataParallel(Backbone_model)\n",
        "\n",
        "  #Linear classifier to train in top of the trained frozen model\n",
        "  Linear = nn.Linear(128, classes).to(device)\n",
        "  optimizer_linear = torch.optim.Adam(Linear.parameters(), lr=lr)\n",
        "  accuracies_linear = []\n",
        "  losses_linear = []\n",
        "\n",
        "\n",
        "  ### Linear evaluation on a randomly initialized model ## (to compare)\n",
        "  if (backbone == \"ResNet\"):\n",
        "    Random_model = ResNet()\n",
        "  else:\n",
        "    Random_model = FCN()\n",
        "\n",
        "  Random_model = Random_model.to(device)\n",
        "  Linear_random = nn.Linear(128, classes).to(device) #Linear classifier to train on top of random model\n",
        "  optimizer_random = torch.optim.Adam(Linear_random.parameters(), lr=lr)\n",
        "  accuracies_random = []\n",
        "  losses_random = []\n",
        "\n",
        "  for e in range(epochs):\n",
        "    Backbone_model.eval()\n",
        "    Linear.train()\n",
        "    loss_list_linear = []\n",
        "\n",
        "    Random_model.eval()\n",
        "    Linear_random.train()\n",
        "    loss_list_random = []\n",
        "\n",
        "    for batch in enumerate(train_loader):\n",
        "      #time_series_batch = batch[1]['time_series']\n",
        "      x1_batch = batch[1]['aug1']\n",
        "      x2_batch = batch[1]['aug2']\n",
        "      label_batch = batch[1]['label']\n",
        "\n",
        "\n",
        "      #Train linear classifier on top of backbone model\n",
        "      #for aug_batch in (x1_batch, x2_batch):\n",
        "      Linear.zero_grad()\n",
        "      with torch.no_grad():\n",
        "        feature1 = Backbone_model(x1_batch.to(device))\n",
        "        feature2 = Backbone_model(x2_batch.to(device))\n",
        "      pred1 = Linear(feature1.to(device))\n",
        "      pred2 = Linear(feature2.to(device))\n",
        "      loss_linear1 = F.cross_entropy(pred1, label_batch)\n",
        "      loss_linear2 = F.cross_entropy(pred2, label_batch)\n",
        "\n",
        "      loss_linear = 0.5*loss_linear1+0.5*loss_linear2\n",
        "      loss_linear.backward()\n",
        "      optimizer_linear.step()\n",
        "      loss_list_linear.append(loss_linear.item())\n",
        "\n",
        "      Linear_random.zero_grad()\n",
        "      with torch.no_grad():\n",
        "        feature_random1 = Random_model(x1_batch.to(device))\n",
        "        feature_random2 = Random_model(x2_batch.to(device))\n",
        "      pred_random1 = Linear_random(feature_random1.to(device))\n",
        "      pred_random2 = Linear_random(feature_random2.to(device))\n",
        "      loss_random1 = F.cross_entropy(pred_random1, label_batch)\n",
        "      loss_random2 = F.cross_entropy(pred_random2, label_batch)\n",
        "\n",
        "      loss_random = 0.5*loss_random1+0.5*loss_random2\n",
        "      loss_random.backward()\n",
        "      optimizer_random.step()\n",
        "      loss_list_random.append(loss_random.item())\n",
        "\n",
        "    Loss_linear = np.mean(loss_list_linear)\n",
        "    losses_linear.append(Loss_linear)\n",
        "    accuracy_linear = evaluate_classifier(test_loader, Backbone_model, Linear)\n",
        "    accuracies_linear.append(accuracy_linear)\n",
        "\n",
        "    Loss_random = np.mean(loss_list_random)\n",
        "    losses_random.append(Loss_random)\n",
        "    accuracy_random = evaluate_classifier(test_loader, Random_model, Linear_random)\n",
        "    accuracies_random.append(accuracy_random)\n",
        "\n",
        "    print(\"e:  \", e, \" dataset: \", dataset,\"  SimSiam:   accuracy; \",accuracy_linear, \" loss; \",Loss_linear, \"   Random : accuracy; \", accuracy_random, \" loss; \", Loss_random)\n",
        "\n",
        "  results = [dataset, \"linear eval\", e , train_bs, lr, accuracies_linear[-1], max(accuracies_linear), losses_linear[-1], max(accuracies_random)]\n",
        "  print(results)\n",
        "\n",
        "  if (SaveName != None):\n",
        "    save_checkpoint_classifier(SaveName, Linear, e, optimizer_linear, losses_linear, accuracies_linear, lr, train_bs) #Save checkpoint\n",
        "  \n",
        "  return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uQzBUYtUpSMf"
      },
      "source": [
        "##### Linear evaluation on each dataset of the big dataset #####\n",
        "\n",
        "#Load trained model\n",
        "Datasets = [\"ChlorineConcentration\", \"ECG5000\", \"ElectricDevices\", \"FordA\", \"FordB\", \"Two_Patterns\", \"wafer\", \"yoga\"]\n",
        "lr_FCN = [ 0.0097,0.0278 ,0.0623 ,0.0689 , 0.0222, 0.0854, 0.0623 ,0.0544]  \n",
        "lr_ResNet = [ 0.0072, 0.0191, 0.0046, 0.0233, 0.0002,0.0370, 0.0265,0.0234]\n",
        "\n",
        "Backbones  = [\"FCN\", \"ResNet\"]\n",
        "\n",
        "batch_size = 40\n",
        "\n",
        "labeled_sizes = [0.2, 0.1, 0.05]\n",
        "\n",
        "rounds = 1\n",
        "epochs = 300\n",
        "\n",
        "\n",
        "Results = []\n",
        "for backbone in Backbones:\n",
        "  if (backbone==\"FCN\"):\n",
        "    #Information to load the saved SimSiam_FCN model from Google drive\n",
        "    SaveName = \"Window38\"\n",
        "    date = 2704\n",
        "    epoch = 99\n",
        "\n",
        "    #Set the Learning rate list as the optimal learning rates previously found by Optuna\n",
        "    lr_list = lr_FCN\n",
        "\n",
        "  else:\n",
        "    #Information to load the saved SimSiam_ResNet model from Google drive\n",
        "    SaveName = \"ResNet_W38\"\n",
        "    date = 2904\n",
        "    epoch = 99\n",
        "\n",
        "    #Set the Learning rate list as the optimal learning rates previously found by Optuna\n",
        "    lr_list = lr_ResNet\n",
        "\n",
        "  ##Loading the pre-trained SimSiam model  \n",
        "  checkpoint = load_checkpoint(SaveName, date, epoch)\n",
        "  trained_model, optimizer_model, epoch_model, lr_model = load_trained_model(checkpoint, backbone=backbone)\n",
        "\n",
        "  for i in range(len(Datasets)):\n",
        "    dataset = Datasets[i]\n",
        "    lr = lr_list[i]\n",
        "\n",
        "    for labeled_size in labeled_sizes:\n",
        "      Train_set = Timeseries_Dataset([dataset], train=True, Save=None, transform = transforms.Compose( [TwoSegments(), ToTensor()] )) #training set loaded from google drive\n",
        "      N_train = len(Train_set)\n",
        "\n",
        "      Train_set.shuffle()\n",
        "      Train_set_labeled = Train_set[:int(N_train*labeled_size)] #labeled set\n",
        "\n",
        "      Test_set = Timeseries_Dataset([dataset], train=False, Save=None, transform = transforms.Compose( [ ToTensor()] )) #test set loaded from google drive\n",
        "\n",
        "      for round in range(rounds):\n",
        "        Train_set_labeled.shuffle()\n",
        "        Test_set.shuffle()\n",
        "        \n",
        "        if (round==1):\n",
        "          Save_checkpoint_name = dataset+str(labeled_size)\n",
        "        else:\n",
        "          Save_checkpoint_name = None\n",
        "\n",
        "        results = Linear_evaluation(trained_model, Train_set_labeled, Test_set, SaveName=Save_checkpoint_name, epochs = epochs , train_bs = batch_size, lr=lr, backbone = backbone)\n",
        "        results.append(SaveName)\n",
        "        results.append(date)\n",
        "        results.append(labeled_size)\n",
        "        \n",
        "        Results.append(results)\n",
        "    \n",
        "    #Saving results to Excel sheet\n",
        "    column_names = [\"Dataset\", \"Type\", \"epochs\", \"batch_size\", \"lr\", \"Accuracy\", \"Max Accuracy\", \"Loss\", \"Random model accuracy\", \"trained model used\", \"date model\", \"labeled size\"]\n",
        "    name = dataset+backbone\n",
        "    to_Excel(Results, column_names, \"lin_eval_\"+name+\".xlsx\")\n",
        "\n",
        "\n",
        "#Saving results to Excel\n",
        "column_names = [\"Dataset\", \"Type\", \"epochs\", \"batch_size\", \"lr\", \"Accuracy\", \"Max Accuracy\", \"Loss\", \"Random model accuracy\", \"trained model used\", \"date model\", \"labeled size\"]\n",
        "name = \"All\"\n",
        "to_Excel(Results, column_names, \"lin_eval_\"+name+\".xlsx\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}